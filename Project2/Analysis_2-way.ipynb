{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f4220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Displays the total number of rows (sentences) in the dataset.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data. \n",
    "# Shuffles the rows in the dataset randomly to ensure randomness in training and testing splits.\n",
    "# frac=1 ensures all rows are included in the shuffled DataFrame.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the statement and tweet columns into a single string for each row.\n",
    "# Example output: A formatted sentence string that concatenates the statement and a related tweet.\n",
    "#sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "\n",
    "df['statement'] = df['statement'].fillna(\"No statement provided\").str.strip()\n",
    "df['tweet'] = df['tweet'].fillna(\"No tweet provided\").str.strip()\n",
    "sentences = 'Statement: ' + df['statement'] + ' || Tweet: ' + df['tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caac61c-165e-4325-83cd-08de139abaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the binary target labels (True or False) for classification.\n",
    "# labels will be used for training the model.\n",
    "# labels = df[\"BinaryNumTarget\"].values\n",
    "assert set(df[\"BinaryNumTarget\"].unique()).issubset({0, 1}), \"Invalid labels detected!\"\n",
    "labels = df[\"BinaryNumTarget\"].fillna(0).astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64918c31-13ca-442b-828c-c975a24b3b2e",
   "metadata": {},
   "source": [
    "### These functions determine the label (truthfulness) for each row based on the combination of the target value and the majority answer.\n",
    "#### Logic for 4-way function:\n",
    "Handles four categories of truthfulness: \"True\", \"False\", \"Mostly True\", \"Mostly False\".\n",
    "The label is flipped if target is False (e.g., \"Agree\" becomes \"False\").\n",
    "#### Logic for 2-way function:\n",
    "Handles binary truthfulness: \"True\" or \"False\".\n",
    "Simplifies the decision by considering only two categories of majority answers: \"Agree\" and \"Disagree\".\n",
    "#### Conditionals: \n",
    "The nested if-elif structure ensures that each row is assigned exactly one label based on its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    # Check if the 'target' column in the current row is True\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly False\"\n",
    "    else:  # If the 'target' column is not True (i.e., False)\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly True\"\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    # Check if the 'target' column in the current row is True\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:  # If the 'target' column is not True (i.e., False)\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1a45-7c95-49df-b0fa-01026c42c601",
   "metadata": {},
   "source": [
    "### This step processes the data for classification tasks:\n",
    "4-way classification for nuanced truthfulness.\n",
    "2-way classification for simpler binary truthfulness.\n",
    "These labels can then be used as target variables for training machine learning or deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = df.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "df2['2-way-label'] = df.apply(lambda x: generate_truthfulness_2way(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45641</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73526</th>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14530</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57568</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44179</th>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31253</th>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88296</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50706</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134198 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label 2-way-label\n",
       "45641   Mostly True        True\n",
       "73526          None        True\n",
       "14530   Mostly True        True\n",
       "18373          True        True\n",
       "57568         False       False\n",
       "...             ...         ...\n",
       "44179          None        True\n",
       "6790           True        True\n",
       "31253          None        True\n",
       "88296         False       False\n",
       "50706  Mostly False       False\n",
       "\n",
       "[134198 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the string labels ('True' and 'False') with numerical values (0 and 1), which are better suited for machine learning models.\n",
    "df2 = df2[df2['2-way-label'].notnull()]\n",
    "df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45641</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73526</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14530</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57568</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44179</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31253</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88296</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50706</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134198 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label  2-way-label\n",
       "45641   Mostly True            0\n",
       "73526          None            0\n",
       "14530   Mostly True            0\n",
       "18373          True            0\n",
       "57568         False            1\n",
       "...             ...          ...\n",
       "44179          None            0\n",
       "6790           True            0\n",
       "31253          None            0\n",
       "88296         False            1\n",
       "50706  Mostly False            1\n",
       "\n",
       "[134198 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        4-way-label  2-way-label\n",
      "45641   Mostly True            0\n",
      "73526          None            0\n",
      "14530   Mostly True            0\n",
      "18373          True            0\n",
      "57568         False            1\n",
      "...             ...          ...\n",
      "44179          None            0\n",
      "6790           True            0\n",
      "31253          None            0\n",
      "88296         False            1\n",
      "50706  Mostly False            1\n",
      "\n",
      "[134198 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a histogram to visualize the distribution of the values in the 2-way-label column.\n",
    "df2['2-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes the numerical values (0 and 1) from the 2-way-label column in df2 and converts them into a NumPy array using the .values attribute.\n",
    "labels = df2['2-way-label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb700d8-a527-4fe6-aeed-ca76ce193e5f",
   "metadata": {},
   "source": [
    "### Loads a pre-trained BERT tokenizer from the Hugging Face model hub.\n",
    "'bert-base-uncased' specifies the version of the tokenizer:\n",
    "bert-base: A smaller, general-purpose BERT model with ~110 million parameters.\n",
    "uncased: The tokenizer will convert all text to lowercase and ignore case distinctions.\n",
    "\n",
    "#### The tokenizer converts input text into tokens (subword units) and maps them to integer IDs based on the BERT vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b229245-2b18-402c-a432-ae9bfb56106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. || Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05edcb-0519-4d9e-a939-0061c23d5861",
   "metadata": {},
   "source": [
    "#### Tokenization: Converts text into a form BERT can process.\n",
    "#### Numerical Mapping: Prepares the data for computation inside the model.\n",
    "#### Understanding: Lets you see exactly how the raw sentence transforms into something the model understands.\n",
    "    \n",
    "### Position in the Sequence:\n",
    "#### [CLS] is always at the start (index 0).\n",
    "##### [SEP] marks the end of a sentence or separates two sentences.\n",
    "### Processing:\n",
    "When BERT processes the input, it assigns special meanings to these tokens:\n",
    "#### The [CLS] token gathers the information from the entire sequence for classification tasks.\n",
    "#### The [SEP] token helps differentiate between sentence segments, improving context understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 134198/134198 [01:14<00:00, 1806.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c454f0d-ca3a-460e-8a26-851806617b87",
   "metadata": {},
   "source": [
    "### This step is foundational for using BERT because it:\n",
    "\n",
    "#### Prepares the data in a numerical, model-compatible format.\n",
    "#### Ensures consistent input length.\n",
    "#### Incorporates special tokens for proper processing by BERT.\n",
    "#### Provides attention masks to focus the model on meaningful parts of the data.\n",
    "Without this preprocessing step, the BERT model would not be able to understand or process your data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbe87e0-a483-4ab3-8670-03b8241fc9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c83d070-225a-42bc-95f1-796879ea899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "text = \"Test sentence for PyTorch tensor\"\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=10,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',  # Ensure PyTorch tensors are requested\n",
    ")\n",
    "\n",
    "print(encoded_dict['input_ids'].device)  # Should print 'cpu' or 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 134198/134198 [01:43<00:00, 1302.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbe85f-034a-4618-a73b-eec9c42f17fd",
   "metadata": {},
   "source": [
    "### PyTorch models expect tensors as inputs. Converting your data into tensors makes it compatible with the model.\n",
    "#### input_ids: A 2D tensor of shape (num_sentences, MAX_SENTENCE_LENGTH) representing tokenized and padded sentences.\n",
    "#### attention_masks: A 2D tensor of the same shape, indicating which tokens are real and which are padding.\n",
    "#### labels: A 1D tensor of shape (num_sentences,) containing binary classification labels.\n",
    "#### This step ensures that all your preprocessed data is ready for use in a PyTorch-based BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "\n",
    "\n",
    "# Ensure input_ids is a list of tensors\n",
    "if not isinstance(input_ids, torch.Tensor):  # Check if input_ids is not already a tensor\n",
    "    input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]  # Convert to tensor list\n",
    "    input_ids = torch.cat(input_ids, dim=0)  # Concatenate tensors along dimension 0\n",
    "\n",
    "# Ensure attention_masks is a list of tensors\n",
    "if not isinstance(attention_masks, torch.Tensor):  # Check if attention_masks is not already a tensor\n",
    "    attention_masks = [torch.tensor(mask, dtype=torch.float) for mask in attention_masks]  # Convert to tensor list\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)  # Concatenate tensors along dimension 0\n",
    "\n",
    "# Ensure labels is a tensor with the correct dtype\n",
    "if not isinstance(labels, torch.Tensor):  # Check if labels is not already a tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)  # Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. || Tweet: BREAKING NEWS: Mitch McConnell accuses President Biden of pushing socialism by implementing the eviction moratorium that will stop millions of Americans from being thrown out on the street this month. RT if you think that Mitch is a heartless idiot!\n",
      "Token IDs: tensor([  101,  4861,  1024,  1996,  2616,  1000,  4942, 28600,  2319, 12256,\n",
      "        17603,  2140,  1010,  1000,  2029,  6945, 16371, 11461,  2170,  2343,\n",
      "        13857,  8112,  1010,  2020,  2109,  2011,  1996, 13157,  2000,  1000,\n",
      "        16114,  1996, 14052,  1997,  1996,  3644,  2451,  1012,  1000,  1064,\n",
      "         1064,  1056, 28394,  2102,  1024,  6945, 16371, 11461, 16360,  5339,\n",
      "        18718,  9706,  4747,  5856,  4371,  1018,  2655,  3653,  2015,  8112,\n",
      "         1000,  4942, 28600,  2319, 12256, 17603,  2140,  1010,  1000, 19067,\n",
      "         1037,  1012,  1043,  1012,  6754, 14455,  2655, 16371, 11461,  1000,\n",
      "         2668,  2567,  1000,  1004, 23713,  1025,  3049,  1059,  2005, 18079,\n",
      "         3835,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Inspects the preprocessed data to ensure that the \n",
    "# tokenization and label preparation steps were done correctly. \n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134198, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fad8b-6dde-4d90-91de-ef51b2f6393c",
   "metadata": {},
   "source": [
    "### This code creates a TensorDataset to combine your inputs (input_ids, attention_masks, and labels) into a single dataset and then splits it into training and validation sets with an 80-20 ratio.\n",
    "#### Prepares Data for Training:\n",
    "Organizes the input IDs, attention masks, and labels into a single structure (TensorDataset) for easy handling.\n",
    "#### Ensures Data Splitting:\n",
    "\n",
    "Divides the dataset into training and validation sets to evaluate the model's performance during training.\n",
    "#### Supports Random Sampling:\n",
    "\n",
    "random_split ensures the split is randomized, which is essential to avoid biases in training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107,358 training samples\n",
      "26,840 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "torch.manual_seed(42)  # Set the random seed\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ed6d9-93c0-43a6-ba0c-bb3da4b30f76",
   "metadata": {},
   "source": [
    "#### This code creates DataLoaders for the training and validation datasets to efficiently handle and feed data to the BERT model during training and evaluation. The DataLoaders manage batching, sampling, and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,                     # The validation dataset\n",
    "    sampler=SequentialSampler(val_dataset),  # Sequential sampling for consistent evaluation\n",
    "    batch_size=batch_size            # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a575d-8a36-4a1e-8434-9d98a5780cc5",
   "metadata": {},
   "source": [
    "#### This code initializes a BERT model for a binary classification task, preparing it for fine-tuning on your dataset. It loads the pre-trained BERT model with a classification head (a linear layer) on top and specifies configurations for the model's behavior and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e860a5a-5757-4df3-98b3-4dc37100852b",
   "metadata": {},
   "source": [
    "#### This code inspects and prints the parameters of the BERT model. It provides insights into the structure and dimensions of the model's parameters, including the embedding layer, transformer layers, and the classification head (output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321e5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used!pip install torch==2.5.1 torchvision transformers==4.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08a049-5551-4b09-84f9-0c7e8ff2dca1",
   "metadata": {},
   "source": [
    "#### This code sets up an optimizer for training your BERT model. Specifically, it uses the AdamW optimizer, which is recommended for fine-tuning transformer models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a2e93-ee7c-4fd9-a1dd-35c1571b6640",
   "metadata": {},
   "source": [
    "#### This code sets up a learning rate scheduler to adjust the learning rate during training. Specifically, it uses a linear schedule with a warmup phase, which is recommended for fine-tuning transformer models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd493e-c762-4efd-9b1d-c3812d744402",
   "metadata": {},
   "source": [
    "#### This code defines a function, flat_accuracy, to calculate the accuracy of predictions compared to the true labels during model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be0a99-907e-4944-b92c-318d1c980761",
   "metadata": {},
   "source": [
    "#### This code defines a utility function format_time that converts a time duration (in seconds) into a more human-readable string format (hh:mm:ss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  changed from /p2/data -> p2/data\n",
    "SAVE_DIR = 'p2/data/checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295f4b2-9830-480d-8879-fb450c0c1fc3",
   "metadata": {},
   "source": [
    "### This code is the training and validation loop for fine-tuning a BERT model on your dataset. It performs the following:\n",
    "\n",
    "#### Trains the model over several epochs.\n",
    "#### Evaluates the model on a validation set after each epoch.\n",
    "#### Tracks key metrics such as training/validation loss, validation accuracy, and timing.\n",
    "#### Saves the model's weights (checkpoints) after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,355.    Elapsed: 0:00:13. Training loss. 0.05325314402580261 Num fake examples 637 Num true examples 643\n",
      "  Batch    80  of  3,355.    Elapsed: 0:00:27. Training loss. 0.04623487591743469 Num fake examples 1263 Num true examples 1297\n",
      "  Batch   120  of  3,355.    Elapsed: 0:00:41. Training loss. 0.045033201575279236 Num fake examples 1903 Num true examples 1937\n",
      "  Batch   160  of  3,355.    Elapsed: 0:00:56. Training loss. 0.34403926134109497 Num fake examples 2472 Num true examples 2648\n",
      "  Batch   200  of  3,355.    Elapsed: 0:01:10. Training loss. 0.4330782890319824 Num fake examples 3089 Num true examples 3311\n",
      "  Batch   240  of  3,355.    Elapsed: 0:01:25. Training loss. 0.1362992227077484 Num fake examples 3702 Num true examples 3978\n",
      "  Batch   280  of  3,355.    Elapsed: 0:01:39. Training loss. 0.34318041801452637 Num fake examples 4338 Num true examples 4622\n",
      "  Batch   320  of  3,355.    Elapsed: 0:01:54. Training loss. 0.44923385977745056 Num fake examples 4973 Num true examples 5267\n",
      "  Batch   360  of  3,355.    Elapsed: 0:02:08. Training loss. 0.13802054524421692 Num fake examples 5574 Num true examples 5946\n",
      "  Batch   400  of  3,355.    Elapsed: 0:02:22. Training loss. 0.03904082626104355 Num fake examples 6211 Num true examples 6589\n",
      "  Batch   440  of  3,355.    Elapsed: 0:02:37. Training loss. 0.13255545496940613 Num fake examples 6820 Num true examples 7260\n",
      "  Batch   480  of  3,355.    Elapsed: 0:02:51. Training loss. 0.2304016500711441 Num fake examples 7445 Num true examples 7915\n",
      "  Batch   520  of  3,355.    Elapsed: 0:03:05. Training loss. 0.24732643365859985 Num fake examples 8056 Num true examples 8584\n",
      "  Batch   560  of  3,355.    Elapsed: 0:03:20. Training loss. 0.21916227042675018 Num fake examples 8639 Num true examples 9281\n",
      "  Batch   600  of  3,355.    Elapsed: 0:03:34. Training loss. 0.2414417713880539 Num fake examples 9242 Num true examples 9958\n",
      "  Batch   640  of  3,355.    Elapsed: 0:03:48. Training loss. 0.2489798665046692 Num fake examples 9899 Num true examples 10581\n",
      "  Batch   680  of  3,355.    Elapsed: 0:04:03. Training loss. 0.2443133145570755 Num fake examples 10529 Num true examples 11231\n",
      "  Batch   720  of  3,355.    Elapsed: 0:04:17. Training loss. 0.348847359418869 Num fake examples 11148 Num true examples 11892\n",
      "  Batch   760  of  3,355.    Elapsed: 0:04:32. Training loss. 0.03675999864935875 Num fake examples 11787 Num true examples 12533\n",
      "  Batch   800  of  3,355.    Elapsed: 0:04:46. Training loss. 0.14411288499832153 Num fake examples 12404 Num true examples 13196\n",
      "  Batch   840  of  3,355.    Elapsed: 0:05:00. Training loss. 0.35191139578819275 Num fake examples 13059 Num true examples 13821\n",
      "  Batch   880  of  3,355.    Elapsed: 0:05:15. Training loss. 0.35155338048934937 Num fake examples 13680 Num true examples 14480\n",
      "  Batch   920  of  3,355.    Elapsed: 0:05:29. Training loss. 0.14330990612506866 Num fake examples 14323 Num true examples 15117\n",
      "  Batch   960  of  3,355.    Elapsed: 0:05:43. Training loss. 0.1385488510131836 Num fake examples 14949 Num true examples 15771\n",
      "  Batch 1,000  of  3,355.    Elapsed: 0:05:58. Training loss. 0.14380037784576416 Num fake examples 15601 Num true examples 16399\n",
      "  Batch 1,040  of  3,355.    Elapsed: 0:06:12. Training loss. 0.22394728660583496 Num fake examples 16240 Num true examples 17040\n",
      "  Batch 1,080  of  3,355.    Elapsed: 0:06:27. Training loss. 0.03967425599694252 Num fake examples 16844 Num true examples 17716\n",
      "  Batch 1,120  of  3,355.    Elapsed: 0:06:41. Training loss. 0.23245097696781158 Num fake examples 17458 Num true examples 18382\n",
      "  Batch 1,160  of  3,355.    Elapsed: 0:06:55. Training loss. 0.03981921821832657 Num fake examples 18072 Num true examples 19048\n",
      "  Batch 1,200  of  3,355.    Elapsed: 0:07:10. Training loss. 0.15270698070526123 Num fake examples 18691 Num true examples 19709\n",
      "  Batch 1,240  of  3,355.    Elapsed: 0:07:24. Training loss. 0.13438370823860168 Num fake examples 19310 Num true examples 20370\n",
      "  Batch 1,280  of  3,355.    Elapsed: 0:07:39. Training loss. 0.12269096076488495 Num fake examples 19914 Num true examples 21046\n",
      "  Batch 1,320  of  3,355.    Elapsed: 0:07:53. Training loss. 0.13771042227745056 Num fake examples 20552 Num true examples 21688\n",
      "  Batch 1,360  of  3,355.    Elapsed: 0:08:07. Training loss. 0.14431773126125336 Num fake examples 21174 Num true examples 22346\n",
      "  Batch 1,400  of  3,355.    Elapsed: 0:08:22. Training loss. 0.1411183774471283 Num fake examples 21812 Num true examples 22988\n",
      "  Batch 1,440  of  3,355.    Elapsed: 0:08:36. Training loss. 0.1309300810098648 Num fake examples 22442 Num true examples 23638\n",
      "  Batch 1,480  of  3,355.    Elapsed: 0:08:50. Training loss. 0.04416937008500099 Num fake examples 23043 Num true examples 24317\n",
      "  Batch 1,520  of  3,355.    Elapsed: 0:09:05. Training loss. 0.05153686925768852 Num fake examples 23674 Num true examples 24966\n",
      "  Batch 1,560  of  3,355.    Elapsed: 0:09:19. Training loss. 0.24323059618473053 Num fake examples 24292 Num true examples 25628\n",
      "  Batch 1,600  of  3,355.    Elapsed: 0:09:34. Training loss. 0.1415989100933075 Num fake examples 24923 Num true examples 26277\n",
      "  Batch 1,640  of  3,355.    Elapsed: 0:09:48. Training loss. 0.14078573882579803 Num fake examples 25553 Num true examples 26927\n",
      "  Batch 1,680  of  3,355.    Elapsed: 0:10:03. Training loss. 0.03978294879198074 Num fake examples 26156 Num true examples 27604\n",
      "  Batch 1,720  of  3,355.    Elapsed: 0:10:17. Training loss. 0.1557288020849228 Num fake examples 26800 Num true examples 28240\n",
      "  Batch 1,760  of  3,355.    Elapsed: 0:10:31. Training loss. 0.14152830839157104 Num fake examples 27432 Num true examples 28888\n",
      "  Batch 1,800  of  3,355.    Elapsed: 0:10:46. Training loss. 0.2352922558784485 Num fake examples 28064 Num true examples 29536\n",
      "  Batch 1,840  of  3,355.    Elapsed: 0:11:00. Training loss. 0.14514034986495972 Num fake examples 28714 Num true examples 30166\n",
      "  Batch 1,880  of  3,355.    Elapsed: 0:11:15. Training loss. 0.13693968951702118 Num fake examples 29328 Num true examples 30832\n",
      "  Batch 1,920  of  3,355.    Elapsed: 0:11:29. Training loss. 0.3216586709022522 Num fake examples 29944 Num true examples 31496\n",
      "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.15134760737419128 Num fake examples 30549 Num true examples 32171\n",
      "  Batch 2,000  of  3,355.    Elapsed: 0:11:58. Training loss. 0.42317187786102295 Num fake examples 31184 Num true examples 32816\n",
      "  Batch 2,040  of  3,355.    Elapsed: 0:12:12. Training loss. 0.25259503722190857 Num fake examples 31794 Num true examples 33486\n",
      "  Batch 2,080  of  3,355.    Elapsed: 0:12:27. Training loss. 0.21366265416145325 Num fake examples 32395 Num true examples 34165\n",
      "  Batch 2,120  of  3,355.    Elapsed: 0:12:41. Training loss. 0.4712429642677307 Num fake examples 32983 Num true examples 34857\n",
      "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.10885500907897949 Num fake examples 33641 Num true examples 35479\n",
      "  Batch 2,200  of  3,355.    Elapsed: 0:13:10. Training loss. 0.047932203859090805 Num fake examples 34282 Num true examples 36118\n",
      "  Batch 2,240  of  3,355.    Elapsed: 0:13:24. Training loss. 0.2473597377538681 Num fake examples 34916 Num true examples 36764\n",
      "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.2504013776779175 Num fake examples 35530 Num true examples 37430\n",
      "  Batch 2,320  of  3,355.    Elapsed: 0:13:53. Training loss. 0.4930192232131958 Num fake examples 36141 Num true examples 38099\n",
      "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.049354881048202515 Num fake examples 36755 Num true examples 38765\n",
      "  Batch 2,400  of  3,355.    Elapsed: 0:14:22. Training loss. 0.2073425054550171 Num fake examples 37349 Num true examples 39451\n",
      "  Batch 2,440  of  3,355.    Elapsed: 0:14:36. Training loss. 0.14362500607967377 Num fake examples 37969 Num true examples 40111\n",
      "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.23683255910873413 Num fake examples 38588 Num true examples 40772\n",
      "  Batch 2,520  of  3,355.    Elapsed: 0:15:05. Training loss. 0.04684709012508392 Num fake examples 39238 Num true examples 41402\n",
      "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.1246090903878212 Num fake examples 39872 Num true examples 42048\n",
      "  Batch 2,600  of  3,355.    Elapsed: 0:15:34. Training loss. 0.20864659547805786 Num fake examples 40468 Num true examples 42732\n",
      "  Batch 2,640  of  3,355.    Elapsed: 0:15:48. Training loss. 0.14996300637722015 Num fake examples 41097 Num true examples 43383\n",
      "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.2482253462076187 Num fake examples 41709 Num true examples 44051\n",
      "  Batch 2,720  of  3,355.    Elapsed: 0:16:17. Training loss. 0.20617693662643433 Num fake examples 42336 Num true examples 44704\n",
      "  Batch 2,760  of  3,355.    Elapsed: 0:16:31. Training loss. 0.21365739405155182 Num fake examples 42958 Num true examples 45362\n",
      "  Batch 2,800  of  3,355.    Elapsed: 0:16:46. Training loss. 0.03816793113946915 Num fake examples 43581 Num true examples 46019\n",
      "  Batch 2,840  of  3,355.    Elapsed: 0:17:00. Training loss. 0.21265727281570435 Num fake examples 44194 Num true examples 46686\n",
      "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.13820533454418182 Num fake examples 44833 Num true examples 47327\n",
      "  Batch 2,920  of  3,355.    Elapsed: 0:17:29. Training loss. 0.25221556425094604 Num fake examples 45428 Num true examples 48012\n",
      "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.14562320709228516 Num fake examples 46051 Num true examples 48669\n",
      "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.04834777116775513 Num fake examples 46672 Num true examples 49328\n",
      "  Batch 3,040  of  3,355.    Elapsed: 0:18:12. Training loss. 0.1346035897731781 Num fake examples 47283 Num true examples 49997\n",
      "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.23583640158176422 Num fake examples 47878 Num true examples 50682\n",
      "  Batch 3,120  of  3,355.    Elapsed: 0:18:41. Training loss. 0.30059054493904114 Num fake examples 48508 Num true examples 51332\n",
      "  Batch 3,160  of  3,355.    Elapsed: 0:18:55. Training loss. 0.0451841726899147 Num fake examples 49124 Num true examples 51996\n",
      "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.3374781608581543 Num fake examples 49758 Num true examples 52642\n",
      "  Batch 3,240  of  3,355.    Elapsed: 0:19:24. Training loss. 0.04189993813633919 Num fake examples 50379 Num true examples 53301\n",
      "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.1328296661376953 Num fake examples 51008 Num true examples 53952\n",
      "  Batch 3,320  of  3,355.    Elapsed: 0:19:53. Training loss. 0.13455358147621155 Num fake examples 51616 Num true examples 54624\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:20:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.03935094550251961 Num fake examples 625 Num true examples 655\n",
      "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.12716390192508698 Num fake examples 1260 Num true examples 1300\n",
      "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.2671760618686676 Num fake examples 1868 Num true examples 1972\n",
      "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.5350862741470337 Num fake examples 2492 Num true examples 2628\n",
      "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.042097486555576324 Num fake examples 3087 Num true examples 3313\n",
      "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1543845236301422 Num fake examples 3726 Num true examples 3954\n",
      "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.14083221554756165 Num fake examples 4329 Num true examples 4631\n",
      "  Batch   320  of  3,355.    Elapsed: 0:01:55. Training loss. 0.03466573730111122 Num fake examples 4962 Num true examples 5278\n",
      "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.33024510741233826 Num fake examples 5571 Num true examples 5949\n",
      "  Batch   400  of  3,355.    Elapsed: 0:02:23. Training loss. 0.32197344303131104 Num fake examples 6164 Num true examples 6636\n",
      "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.11075636744499207 Num fake examples 6811 Num true examples 7269\n",
      "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.13908791542053223 Num fake examples 7466 Num true examples 7894\n",
      "  Batch   520  of  3,355.    Elapsed: 0:03:07. Training loss. 0.4561178982257843 Num fake examples 8066 Num true examples 8574\n",
      "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.1121193915605545 Num fake examples 8687 Num true examples 9233\n",
      "  Batch   600  of  3,355.    Elapsed: 0:03:35. Training loss. 0.25839942693710327 Num fake examples 9303 Num true examples 9897\n",
      "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.04425102472305298 Num fake examples 9937 Num true examples 10543\n",
      "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.3780013620853424 Num fake examples 10577 Num true examples 11183\n",
      "  Batch   720  of  3,355.    Elapsed: 0:04:19. Training loss. 0.133817657828331 Num fake examples 11185 Num true examples 11855\n",
      "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.041212279349565506 Num fake examples 11836 Num true examples 12484\n",
      "  Batch   800  of  3,355.    Elapsed: 0:04:47. Training loss. 0.24937984347343445 Num fake examples 12437 Num true examples 13163\n",
      "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.04223354160785675 Num fake examples 13058 Num true examples 13822\n",
      "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.03940189629793167 Num fake examples 13663 Num true examples 14497\n",
      "  Batch   920  of  3,355.    Elapsed: 0:05:30. Training loss. 0.12211575359106064 Num fake examples 14299 Num true examples 15141\n",
      "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.2688104212284088 Num fake examples 14920 Num true examples 15800\n",
      "  Batch 1,000  of  3,355.    Elapsed: 0:05:59. Training loss. 0.03407096117734909 Num fake examples 15545 Num true examples 16455\n",
      "  Batch 1,040  of  3,355.    Elapsed: 0:06:14. Training loss. 0.3238053321838379 Num fake examples 16171 Num true examples 17109\n",
      "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.25952139496803284 Num fake examples 16818 Num true examples 17742\n",
      "  Batch 1,120  of  3,355.    Elapsed: 0:06:42. Training loss. 0.11196809262037277 Num fake examples 17436 Num true examples 18404\n",
      "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.23539024591445923 Num fake examples 18036 Num true examples 19084\n",
      "  Batch 1,200  of  3,355.    Elapsed: 0:07:11. Training loss. 0.03345056250691414 Num fake examples 18676 Num true examples 19724\n",
      "  Batch 1,240  of  3,355.    Elapsed: 0:07:26. Training loss. 0.21026352047920227 Num fake examples 19284 Num true examples 20396\n",
      "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.03577909246087074 Num fake examples 19907 Num true examples 21053\n",
      "  Batch 1,320  of  3,355.    Elapsed: 0:07:54. Training loss. 0.11131535470485687 Num fake examples 20548 Num true examples 21692\n",
      "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.39729541540145874 Num fake examples 21170 Num true examples 22350\n",
      "  Batch 1,400  of  3,355.    Elapsed: 0:08:23. Training loss. 0.10656000673770905 Num fake examples 21795 Num true examples 23005\n",
      "  Batch 1,440  of  3,355.    Elapsed: 0:08:37. Training loss. 0.15148267149925232 Num fake examples 22403 Num true examples 23677\n",
      "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.04705808684229851 Num fake examples 23084 Num true examples 24276\n",
      "  Batch 1,520  of  3,355.    Elapsed: 0:09:06. Training loss. 0.14977127313613892 Num fake examples 23704 Num true examples 24936\n",
      "  Batch 1,560  of  3,355.    Elapsed: 0:09:20. Training loss. 0.03257656842470169 Num fake examples 24361 Num true examples 25559\n",
      "  Batch 1,600  of  3,355.    Elapsed: 0:09:35. Training loss. 0.13818959891796112 Num fake examples 25013 Num true examples 26187\n",
      "  Batch 1,640  of  3,355.    Elapsed: 0:09:49. Training loss. 0.02896132692694664 Num fake examples 25619 Num true examples 26861\n",
      "  Batch 1,680  of  3,355.    Elapsed: 0:10:04. Training loss. 0.2429731786251068 Num fake examples 26224 Num true examples 27536\n",
      "  Batch 1,720  of  3,355.    Elapsed: 0:10:18. Training loss. 0.19558961689472198 Num fake examples 26849 Num true examples 28191\n",
      "  Batch 1,760  of  3,355.    Elapsed: 0:10:32. Training loss. 0.1265896111726761 Num fake examples 27488 Num true examples 28832\n",
      "  Batch 1,800  of  3,355.    Elapsed: 0:10:47. Training loss. 0.14584015309810638 Num fake examples 28104 Num true examples 29496\n",
      "  Batch 1,840  of  3,355.    Elapsed: 0:11:01. Training loss. 0.11036056280136108 Num fake examples 28715 Num true examples 30165\n",
      "  Batch 1,880  of  3,355.    Elapsed: 0:11:15. Training loss. 0.2579878568649292 Num fake examples 29374 Num true examples 30786\n",
      "  Batch 1,920  of  3,355.    Elapsed: 0:11:30. Training loss. 0.1441950649023056 Num fake examples 29989 Num true examples 31451\n",
      "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.04405297338962555 Num fake examples 30605 Num true examples 32115\n",
      "  Batch 2,000  of  3,355.    Elapsed: 0:11:59. Training loss. 0.1581941694021225 Num fake examples 31247 Num true examples 32753\n",
      "  Batch 2,040  of  3,355.    Elapsed: 0:12:13. Training loss. 0.31049904227256775 Num fake examples 31851 Num true examples 33429\n",
      "  Batch 2,080  of  3,355.    Elapsed: 0:12:27. Training loss. 0.2443661093711853 Num fake examples 32466 Num true examples 34094\n",
      "  Batch 2,120  of  3,355.    Elapsed: 0:12:42. Training loss. 0.14016607403755188 Num fake examples 33066 Num true examples 34774\n",
      "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.04870529845356941 Num fake examples 33705 Num true examples 35415\n",
      "  Batch 2,200  of  3,355.    Elapsed: 0:13:10. Training loss. 0.11456242203712463 Num fake examples 34303 Num true examples 36097\n",
      "  Batch 2,240  of  3,355.    Elapsed: 0:13:25. Training loss. 0.27177000045776367 Num fake examples 34916 Num true examples 36764\n",
      "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.03737764060497284 Num fake examples 35527 Num true examples 37433\n",
      "  Batch 2,320  of  3,355.    Elapsed: 0:13:54. Training loss. 0.13009679317474365 Num fake examples 36181 Num true examples 38059\n",
      "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.05307362228631973 Num fake examples 36791 Num true examples 38729\n",
      "  Batch 2,400  of  3,355.    Elapsed: 0:14:22. Training loss. 0.26379427313804626 Num fake examples 37420 Num true examples 39380\n",
      "  Batch 2,440  of  3,355.    Elapsed: 0:14:37. Training loss. 0.2513780891895294 Num fake examples 38038 Num true examples 40042\n",
      "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.23142875730991364 Num fake examples 38639 Num true examples 40721\n",
      "  Batch 2,520  of  3,355.    Elapsed: 0:15:05. Training loss. 0.1414983719587326 Num fake examples 39221 Num true examples 41419\n",
      "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.23584167659282684 Num fake examples 39835 Num true examples 42085\n",
      "  Batch 2,600  of  3,355.    Elapsed: 0:15:34. Training loss. 0.2483856976032257 Num fake examples 40462 Num true examples 42738\n",
      "  Batch 2,640  of  3,355.    Elapsed: 0:15:49. Training loss. 0.24674564599990845 Num fake examples 41046 Num true examples 43434\n",
      "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.22001497447490692 Num fake examples 41684 Num true examples 44076\n",
      "  Batch 2,720  of  3,355.    Elapsed: 0:16:17. Training loss. 0.14037881791591644 Num fake examples 42318 Num true examples 44722\n",
      "  Batch 2,760  of  3,355.    Elapsed: 0:16:32. Training loss. 0.043773286044597626 Num fake examples 42939 Num true examples 45381\n",
      "  Batch 2,800  of  3,355.    Elapsed: 0:16:46. Training loss. 0.04199671000242233 Num fake examples 43580 Num true examples 46020\n",
      "  Batch 2,840  of  3,355.    Elapsed: 0:17:00. Training loss. 0.25485843420028687 Num fake examples 44199 Num true examples 46681\n",
      "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.3623812198638916 Num fake examples 44826 Num true examples 47334\n",
      "  Batch 2,920  of  3,355.    Elapsed: 0:17:29. Training loss. 0.20391181111335754 Num fake examples 45437 Num true examples 48003\n",
      "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.3336174488067627 Num fake examples 46064 Num true examples 48656\n",
      "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.03978920355439186 Num fake examples 46680 Num true examples 49320\n",
      "  Batch 3,040  of  3,355.    Elapsed: 0:18:12. Training loss. 0.231441929936409 Num fake examples 47307 Num true examples 49973\n",
      "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.2307634800672531 Num fake examples 47935 Num true examples 50625\n",
      "  Batch 3,120  of  3,355.    Elapsed: 0:18:41. Training loss. 0.25378817319869995 Num fake examples 48550 Num true examples 51290\n",
      "  Batch 3,160  of  3,355.    Elapsed: 0:18:56. Training loss. 0.3293209671974182 Num fake examples 49164 Num true examples 51956\n",
      "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.03976007550954819 Num fake examples 49797 Num true examples 52603\n",
      "  Batch 3,240  of  3,355.    Elapsed: 0:19:24. Training loss. 0.328784704208374 Num fake examples 50425 Num true examples 53255\n",
      "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.03953807055950165 Num fake examples 51025 Num true examples 53935\n",
      "  Batch 3,320  of  3,355.    Elapsed: 0:19:53. Training loss. 0.43083977699279785 Num fake examples 51634 Num true examples 54606\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:20:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.3338867425918579 Num fake examples 633 Num true examples 647\n",
      "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.24493588507175446 Num fake examples 1238 Num true examples 1322\n",
      "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.23917953670024872 Num fake examples 1862 Num true examples 1978\n",
      "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.04362295940518379 Num fake examples 2476 Num true examples 2644\n",
      "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.13870671391487122 Num fake examples 3082 Num true examples 3318\n",
      "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1341569870710373 Num fake examples 3708 Num true examples 3972\n",
      "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.13023115694522858 Num fake examples 4292 Num true examples 4668\n",
      "  Batch   320  of  3,355.    Elapsed: 0:01:54. Training loss. 0.2368895709514618 Num fake examples 4901 Num true examples 5339\n",
      "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.2234560251235962 Num fake examples 5509 Num true examples 6011\n",
      "  Batch   400  of  3,355.    Elapsed: 0:02:23. Training loss. 0.12225377559661865 Num fake examples 6113 Num true examples 6687\n",
      "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.22883129119873047 Num fake examples 6707 Num true examples 7373\n",
      "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.0419744998216629 Num fake examples 7328 Num true examples 8032\n",
      "  Batch   520  of  3,355.    Elapsed: 0:03:06. Training loss. 0.13669711351394653 Num fake examples 7955 Num true examples 8685\n",
      "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.04214046150445938 Num fake examples 8557 Num true examples 9363\n",
      "  Batch   600  of  3,355.    Elapsed: 0:03:35. Training loss. 0.04172424599528313 Num fake examples 9160 Num true examples 10040\n",
      "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.2394254207611084 Num fake examples 9781 Num true examples 10699\n",
      "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.13004055619239807 Num fake examples 10439 Num true examples 11321\n",
      "  Batch   720  of  3,355.    Elapsed: 0:04:18. Training loss. 0.03967522829771042 Num fake examples 11048 Num true examples 11992\n",
      "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.334077924489975 Num fake examples 11655 Num true examples 12665\n",
      "  Batch   800  of  3,355.    Elapsed: 0:04:47. Training loss. 0.214199960231781 Num fake examples 12267 Num true examples 13333\n",
      "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.21729257702827454 Num fake examples 12916 Num true examples 13964\n",
      "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.1504918783903122 Num fake examples 13539 Num true examples 14621\n",
      "  Batch   920  of  3,355.    Elapsed: 0:05:30. Training loss. 0.13153523206710815 Num fake examples 14158 Num true examples 15282\n",
      "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.04383350908756256 Num fake examples 14743 Num true examples 15977\n",
      "  Batch 1,000  of  3,355.    Elapsed: 0:05:59. Training loss. 0.03426191210746765 Num fake examples 15365 Num true examples 16635\n",
      "  Batch 1,040  of  3,355.    Elapsed: 0:06:13. Training loss. 0.03896559774875641 Num fake examples 15988 Num true examples 17292\n",
      "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.13873271644115448 Num fake examples 16614 Num true examples 17946\n",
      "  Batch 1,120  of  3,355.    Elapsed: 0:06:42. Training loss. 0.1565924882888794 Num fake examples 17269 Num true examples 18571\n",
      "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.11483988910913467 Num fake examples 17901 Num true examples 19219\n",
      "  Batch 1,200  of  3,355.    Elapsed: 0:07:11. Training loss. 0.0364774614572525 Num fake examples 18506 Num true examples 19894\n",
      "  Batch 1,240  of  3,355.    Elapsed: 0:07:25. Training loss. 0.036551475524902344 Num fake examples 19158 Num true examples 20522\n",
      "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.03574036806821823 Num fake examples 19761 Num true examples 21199\n",
      "  Batch 1,320  of  3,355.    Elapsed: 0:07:54. Training loss. 0.14958828687667847 Num fake examples 20374 Num true examples 21866\n",
      "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.12117072194814682 Num fake examples 20980 Num true examples 22540\n",
      "  Batch 1,400  of  3,355.    Elapsed: 0:08:23. Training loss. 0.037765443325042725 Num fake examples 21625 Num true examples 23175\n",
      "  Batch 1,440  of  3,355.    Elapsed: 0:08:37. Training loss. 0.0368092879652977 Num fake examples 22254 Num true examples 23826\n",
      "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.2496054470539093 Num fake examples 22902 Num true examples 24458\n",
      "  Batch 1,520  of  3,355.    Elapsed: 0:09:06. Training loss. 0.2884432375431061 Num fake examples 23515 Num true examples 25125\n",
      "  Batch 1,560  of  3,355.    Elapsed: 0:09:20. Training loss. 0.3019912838935852 Num fake examples 24144 Num true examples 25776\n",
      "  Batch 1,600  of  3,355.    Elapsed: 0:09:35. Training loss. 0.23881559073925018 Num fake examples 24764 Num true examples 26436\n",
      "  Batch 1,640  of  3,355.    Elapsed: 0:09:49. Training loss. 0.13872145116329193 Num fake examples 25405 Num true examples 27075\n",
      "  Batch 1,680  of  3,355.    Elapsed: 0:10:04. Training loss. 0.22398824989795685 Num fake examples 26023 Num true examples 27737\n",
      "  Batch 1,720  of  3,355.    Elapsed: 0:10:18. Training loss. 0.037736695259809494 Num fake examples 26656 Num true examples 28384\n",
      "  Batch 1,760  of  3,355.    Elapsed: 0:10:32. Training loss. 0.1363040655851364 Num fake examples 27294 Num true examples 29026\n",
      "  Batch 1,800  of  3,355.    Elapsed: 0:10:47. Training loss. 0.25522521138191223 Num fake examples 27911 Num true examples 29689\n",
      "  Batch 1,840  of  3,355.    Elapsed: 0:11:01. Training loss. 0.13717131316661835 Num fake examples 28525 Num true examples 30355\n",
      "  Batch 1,880  of  3,355.    Elapsed: 0:11:16. Training loss. 0.1466633826494217 Num fake examples 29164 Num true examples 30996\n",
      "  Batch 1,920  of  3,355.    Elapsed: 0:11:30. Training loss. 0.04241253063082695 Num fake examples 29775 Num true examples 31665\n",
      "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.1977137327194214 Num fake examples 30384 Num true examples 32336\n",
      "  Batch 2,000  of  3,355.    Elapsed: 0:11:59. Training loss. 0.2531745433807373 Num fake examples 30992 Num true examples 33008\n",
      "  Batch 2,040  of  3,355.    Elapsed: 0:12:13. Training loss. 0.04097733274102211 Num fake examples 31652 Num true examples 33628\n",
      "  Batch 2,080  of  3,355.    Elapsed: 0:12:28. Training loss. 0.040315769612789154 Num fake examples 32286 Num true examples 34274\n",
      "  Batch 2,120  of  3,355.    Elapsed: 0:12:42. Training loss. 0.26836201548576355 Num fake examples 32926 Num true examples 34914\n",
      "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.1515481173992157 Num fake examples 33532 Num true examples 35588\n",
      "  Batch 2,200  of  3,355.    Elapsed: 0:13:11. Training loss. 0.04283876344561577 Num fake examples 34138 Num true examples 36262\n",
      "  Batch 2,240  of  3,355.    Elapsed: 0:13:25. Training loss. 0.12743979692459106 Num fake examples 34753 Num true examples 36927\n",
      "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.03976508229970932 Num fake examples 35372 Num true examples 37588\n",
      "  Batch 2,320  of  3,355.    Elapsed: 0:13:54. Training loss. 0.15163537859916687 Num fake examples 36002 Num true examples 38238\n",
      "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.32829177379608154 Num fake examples 36641 Num true examples 38879\n",
      "  Batch 2,400  of  3,355.    Elapsed: 0:14:23. Training loss. 0.31207674741744995 Num fake examples 37299 Num true examples 39501\n",
      "  Batch 2,440  of  3,355.    Elapsed: 0:14:37. Training loss. 0.11240227520465851 Num fake examples 37915 Num true examples 40165\n",
      "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.11144663393497467 Num fake examples 38575 Num true examples 40785\n",
      "  Batch 2,520  of  3,355.    Elapsed: 0:15:06. Training loss. 0.23262450098991394 Num fake examples 39212 Num true examples 41428\n",
      "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.24706904590129852 Num fake examples 39823 Num true examples 42097\n",
      "  Batch 2,600  of  3,355.    Elapsed: 0:15:35. Training loss. 0.11494562774896622 Num fake examples 40406 Num true examples 42794\n",
      "  Batch 2,640  of  3,355.    Elapsed: 0:15:49. Training loss. 0.3657858967781067 Num fake examples 41031 Num true examples 43449\n",
      "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.16093550622463226 Num fake examples 41654 Num true examples 44106\n",
      "  Batch 2,720  of  3,355.    Elapsed: 0:16:18. Training loss. 0.04275315999984741 Num fake examples 42280 Num true examples 44760\n",
      "  Batch 2,760  of  3,355.    Elapsed: 0:16:32. Training loss. 0.1517629623413086 Num fake examples 42882 Num true examples 45438\n",
      "  Batch 2,800  of  3,355.    Elapsed: 0:16:47. Training loss. 0.15819057822227478 Num fake examples 43529 Num true examples 46071\n",
      "  Batch 2,840  of  3,355.    Elapsed: 0:17:01. Training loss. 0.22100567817687988 Num fake examples 44139 Num true examples 46741\n",
      "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.23929274082183838 Num fake examples 44770 Num true examples 47390\n",
      "  Batch 2,920  of  3,355.    Elapsed: 0:17:30. Training loss. 0.4428822994232178 Num fake examples 45425 Num true examples 48015\n",
      "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.3581375479698181 Num fake examples 46026 Num true examples 48694\n",
      "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.22258593142032623 Num fake examples 46649 Num true examples 49351\n",
      "  Batch 3,040  of  3,355.    Elapsed: 0:18:13. Training loss. 0.24387116730213165 Num fake examples 47274 Num true examples 50006\n",
      "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.2356257438659668 Num fake examples 47911 Num true examples 50649\n",
      "  Batch 3,120  of  3,355.    Elapsed: 0:18:42. Training loss. 0.4521031677722931 Num fake examples 48541 Num true examples 51299\n",
      "  Batch 3,160  of  3,355.    Elapsed: 0:18:56. Training loss. 0.13446354866027832 Num fake examples 49144 Num true examples 51976\n",
      "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.03812853619456291 Num fake examples 49787 Num true examples 52613\n",
      "  Batch 3,240  of  3,355.    Elapsed: 0:19:25. Training loss. 0.043760865926742554 Num fake examples 50413 Num true examples 53267\n",
      "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.1384311467409134 Num fake examples 51017 Num true examples 53943\n",
      "  Batch 3,320  of  3,355.    Elapsed: 0:19:54. Training loss. 0.13893994688987732 Num fake examples 51629 Num true examples 54611\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:20:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.2409074604511261 Num fake examples 625 Num true examples 655\n",
      "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.03874039649963379 Num fake examples 1240 Num true examples 1320\n",
      "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.23137454688549042 Num fake examples 1864 Num true examples 1976\n",
      "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.04342831298708916 Num fake examples 2494 Num true examples 2626\n",
      "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.043720610439777374 Num fake examples 3124 Num true examples 3276\n",
      "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1579982191324234 Num fake examples 3738 Num true examples 3942\n",
      "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.037048667669296265 Num fake examples 4352 Num true examples 4608\n",
      "  Batch   320  of  3,355.    Elapsed: 0:01:55. Training loss. 0.03720291703939438 Num fake examples 4987 Num true examples 5253\n",
      "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.10871105641126633 Num fake examples 5630 Num true examples 5890\n",
      "  Batch   400  of  3,355.    Elapsed: 0:02:24. Training loss. 0.037721361964941025 Num fake examples 6245 Num true examples 6555\n",
      "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.3096107244491577 Num fake examples 6869 Num true examples 7211\n",
      "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.14429622888565063 Num fake examples 7470 Num true examples 7890\n",
      "  Batch   520  of  3,355.    Elapsed: 0:03:07. Training loss. 0.1458272784948349 Num fake examples 8112 Num true examples 8528\n",
      "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.10995736718177795 Num fake examples 8746 Num true examples 9174\n",
      "  Batch   600  of  3,355.    Elapsed: 0:03:36. Training loss. 0.03944821655750275 Num fake examples 9396 Num true examples 9804\n",
      "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.3315431773662567 Num fake examples 10017 Num true examples 10463\n",
      "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.23462273180484772 Num fake examples 10641 Num true examples 11119\n",
      "  Batch   720  of  3,355.    Elapsed: 0:04:19. Training loss. 0.16211122274398804 Num fake examples 11242 Num true examples 11798\n",
      "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.03663318604230881 Num fake examples 11902 Num true examples 12418\n",
      "  Batch   800  of  3,355.    Elapsed: 0:04:48. Training loss. 0.23556575179100037 Num fake examples 12510 Num true examples 13090\n",
      "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.2568124830722809 Num fake examples 13147 Num true examples 13733\n",
      "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.41200733184814453 Num fake examples 13765 Num true examples 14395\n",
      "  Batch   920  of  3,355.    Elapsed: 0:05:31. Training loss. 0.23185721039772034 Num fake examples 14424 Num true examples 15016\n",
      "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.041035495698451996 Num fake examples 15065 Num true examples 15655\n",
      "  Batch 1,000  of  3,355.    Elapsed: 0:06:00. Training loss. 0.23801107704639435 Num fake examples 15668 Num true examples 16332\n",
      "  Batch 1,040  of  3,355.    Elapsed: 0:06:14. Training loss. 0.15904563665390015 Num fake examples 16286 Num true examples 16994\n",
      "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.2100268006324768 Num fake examples 16902 Num true examples 17658\n",
      "  Batch 1,120  of  3,355.    Elapsed: 0:06:43. Training loss. 0.24760288000106812 Num fake examples 17516 Num true examples 18324\n",
      "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.04195927083492279 Num fake examples 18107 Num true examples 19013\n",
      "  Batch 1,200  of  3,355.    Elapsed: 0:07:12. Training loss. 0.2511937916278839 Num fake examples 18718 Num true examples 19682\n",
      "  Batch 1,240  of  3,355.    Elapsed: 0:07:26. Training loss. 0.03692358732223511 Num fake examples 19328 Num true examples 20352\n",
      "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.04175263270735741 Num fake examples 19982 Num true examples 20978\n",
      "  Batch 1,320  of  3,355.    Elapsed: 0:07:55. Training loss. 0.1322416216135025 Num fake examples 20580 Num true examples 21660\n",
      "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.1274494081735611 Num fake examples 21193 Num true examples 22327\n",
      "  Batch 1,400  of  3,355.    Elapsed: 0:08:24. Training loss. 0.1172788217663765 Num fake examples 21816 Num true examples 22984\n",
      "  Batch 1,440  of  3,355.    Elapsed: 0:08:38. Training loss. 0.24210676550865173 Num fake examples 22417 Num true examples 23663\n",
      "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.03667015582323074 Num fake examples 23047 Num true examples 24313\n",
      "  Batch 1,520  of  3,355.    Elapsed: 0:09:07. Training loss. 0.31395086646080017 Num fake examples 23650 Num true examples 24990\n",
      "  Batch 1,560  of  3,355.    Elapsed: 0:09:21. Training loss. 0.16145384311676025 Num fake examples 24278 Num true examples 25642\n",
      "  Batch 1,600  of  3,355.    Elapsed: 0:09:36. Training loss. 0.15182346105575562 Num fake examples 24877 Num true examples 26323\n",
      "  Batch 1,640  of  3,355.    Elapsed: 0:09:50. Training loss. 0.21085284650325775 Num fake examples 25514 Num true examples 26966\n",
      "  Batch 1,680  of  3,355.    Elapsed: 0:10:05. Training loss. 0.1323704868555069 Num fake examples 26101 Num true examples 27659\n",
      "  Batch 1,720  of  3,355.    Elapsed: 0:10:19. Training loss. 0.1545555293560028 Num fake examples 26734 Num true examples 28306\n",
      "  Batch 1,760  of  3,355.    Elapsed: 0:10:33. Training loss. 0.13633529841899872 Num fake examples 27362 Num true examples 28958\n",
      "  Batch 1,800  of  3,355.    Elapsed: 0:10:48. Training loss. 0.136884868144989 Num fake examples 28005 Num true examples 29595\n",
      "  Batch 1,840  of  3,355.    Elapsed: 0:11:02. Training loss. 0.12205293774604797 Num fake examples 28596 Num true examples 30284\n",
      "  Batch 1,880  of  3,355.    Elapsed: 0:11:17. Training loss. 0.23902100324630737 Num fake examples 29193 Num true examples 30967\n",
      "  Batch 1,920  of  3,355.    Elapsed: 0:11:31. Training loss. 0.15080493688583374 Num fake examples 29822 Num true examples 31618\n",
      "  Batch 1,960  of  3,355.    Elapsed: 0:11:45. Training loss. 0.047677673399448395 Num fake examples 30434 Num true examples 32286\n",
      "  Batch 2,000  of  3,355.    Elapsed: 0:12:00. Training loss. 0.03955874219536781 Num fake examples 31074 Num true examples 32926\n",
      "  Batch 2,040  of  3,355.    Elapsed: 0:12:14. Training loss. 0.23810961842536926 Num fake examples 31682 Num true examples 33598\n",
      "  Batch 2,080  of  3,355.    Elapsed: 0:12:29. Training loss. 0.24548694491386414 Num fake examples 32316 Num true examples 34244\n",
      "  Batch 2,120  of  3,355.    Elapsed: 0:12:43. Training loss. 0.3976697623729706 Num fake examples 32946 Num true examples 34894\n",
      "  Batch 2,160  of  3,355.    Elapsed: 0:12:57. Training loss. 0.043257374316453934 Num fake examples 33563 Num true examples 35557\n",
      "  Batch 2,200  of  3,355.    Elapsed: 0:13:12. Training loss. 0.03786439076066017 Num fake examples 34201 Num true examples 36199\n",
      "  Batch 2,240  of  3,355.    Elapsed: 0:13:26. Training loss. 0.03973526880145073 Num fake examples 34815 Num true examples 36865\n",
      "  Batch 2,280  of  3,355.    Elapsed: 0:13:41. Training loss. 0.32164761424064636 Num fake examples 35448 Num true examples 37512\n",
      "  Batch 2,320  of  3,355.    Elapsed: 0:13:55. Training loss. 0.1384255588054657 Num fake examples 36051 Num true examples 38189\n",
      "  Batch 2,360  of  3,355.    Elapsed: 0:14:10. Training loss. 0.14196622371673584 Num fake examples 36663 Num true examples 38857\n",
      "  Batch 2,400  of  3,355.    Elapsed: 0:14:24. Training loss. 0.24955777823925018 Num fake examples 37285 Num true examples 39515\n",
      "  Batch 2,440  of  3,355.    Elapsed: 0:14:38. Training loss. 0.14557817578315735 Num fake examples 37908 Num true examples 40172\n",
      "  Batch 2,480  of  3,355.    Elapsed: 0:14:53. Training loss. 0.23301628232002258 Num fake examples 38539 Num true examples 40821\n",
      "  Batch 2,520  of  3,355.    Elapsed: 0:15:07. Training loss. 0.04076351225376129 Num fake examples 39178 Num true examples 41462\n",
      "  Batch 2,560  of  3,355.    Elapsed: 0:15:22. Training loss. 0.04160863161087036 Num fake examples 39790 Num true examples 42130\n",
      "  Batch 2,600  of  3,355.    Elapsed: 0:15:36. Training loss. 0.038130082190036774 Num fake examples 40434 Num true examples 42766\n",
      "  Batch 2,640  of  3,355.    Elapsed: 0:15:50. Training loss. 0.15199041366577148 Num fake examples 41086 Num true examples 43394\n",
      "  Batch 2,680  of  3,355.    Elapsed: 0:16:05. Training loss. 0.04173336550593376 Num fake examples 41713 Num true examples 44047\n",
      "  Batch 2,720  of  3,355.    Elapsed: 0:16:19. Training loss. 0.04318900778889656 Num fake examples 42331 Num true examples 44709\n",
      "  Batch 2,760  of  3,355.    Elapsed: 0:16:34. Training loss. 0.13546667993068695 Num fake examples 42956 Num true examples 45364\n",
      "  Batch 2,800  of  3,355.    Elapsed: 0:16:48. Training loss. 0.04681965708732605 Num fake examples 43567 Num true examples 46033\n",
      "  Batch 2,840  of  3,355.    Elapsed: 0:17:03. Training loss. 0.248200923204422 Num fake examples 44209 Num true examples 46671\n",
      "  Batch 2,880  of  3,355.    Elapsed: 0:17:17. Training loss. 0.31713253259658813 Num fake examples 44816 Num true examples 47344\n",
      "  Batch 2,920  of  3,355.    Elapsed: 0:17:31. Training loss. 0.20648923516273499 Num fake examples 45438 Num true examples 48002\n",
      "  Batch 2,960  of  3,355.    Elapsed: 0:17:46. Training loss. 0.14694374799728394 Num fake examples 46055 Num true examples 48665\n",
      "  Batch 3,000  of  3,355.    Elapsed: 0:18:00. Training loss. 0.12243640422821045 Num fake examples 46677 Num true examples 49323\n",
      "  Batch 3,040  of  3,355.    Elapsed: 0:18:15. Training loss. 0.4050387144088745 Num fake examples 47276 Num true examples 50004\n",
      "  Batch 3,080  of  3,355.    Elapsed: 0:18:29. Training loss. 0.04104367271065712 Num fake examples 47902 Num true examples 50658\n",
      "  Batch 3,120  of  3,355.    Elapsed: 0:18:43. Training loss. 0.043901000171899796 Num fake examples 48506 Num true examples 51334\n",
      "  Batch 3,160  of  3,355.    Elapsed: 0:18:58. Training loss. 0.2628294825553894 Num fake examples 49147 Num true examples 51973\n",
      "  Batch 3,200  of  3,355.    Elapsed: 0:19:12. Training loss. 0.23191672563552856 Num fake examples 49770 Num true examples 52630\n",
      "  Batch 3,240  of  3,355.    Elapsed: 0:19:27. Training loss. 0.22508689761161804 Num fake examples 50398 Num true examples 53282\n",
      "  Batch 3,280  of  3,355.    Elapsed: 0:19:41. Training loss. 0.35537004470825195 Num fake examples 51025 Num true examples 53935\n",
      "  Batch 3,320  of  3,355.    Elapsed: 0:19:55. Training loss. 0.04083313047885895 Num fake examples 51636 Num true examples 54604\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:20:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:28:33 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 8000:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += (b_labels == 1).sum().item()\n",
    "        total_true_examples += (b_labels == 0).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        if step > 8000:\n",
    "            break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7478b7d-8807-45ec-8269-c0a3e01091c3",
   "metadata": {},
   "source": [
    "#### This code encodes a single sentence, passes it through the fine-tuned BERT model in evaluation mode, and prints the model's output along with the true label for that sentence. The purpose is to test the model on a specific input sentence and examine its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.4051, -5.4143]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6035, -0.5707]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.3777, -5.3881],\n",
      "        [ 5.3652, -5.3792]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[ 5.0588, -5.0063],\n",
      "        [ 5.3725, -5.4029]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[-5.3577,  5.3462],\n",
      "        [-5.3761,  5.3724]], device='cuda:0') tensor([0, 1], device='cuda:0')\n",
      "tensor([[ 5.4580, -5.4627],\n",
      "        [ 5.4304, -5.4484]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[-5.3407,  5.3387],\n",
      "        [ 4.5246, -4.5115]], device='cuda:0') tensor([1, 0], device='cuda:0')\n",
      "tensor([[-5.3731,  5.3666],\n",
      "        [-5.3586,  5.3509]], device='cuda:0') tensor([1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
