{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019745dc-4c9f-4c4c-b010-c4a8365e5ab8",
   "metadata": {},
   "source": [
    "# Baseline results 2-way-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81923d-fadb-41b4-aaff-3996f91deea3",
   "metadata": {},
   "source": [
    "```\n",
    "======== Epoch 1 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  53,679.    Elapsed: 0:00:02. Training loss. 0.7096359729766846 Num fake examples 42 Num true examples 38\n",
    "  Batch    80  of  53,679.    Elapsed: 0:00:03. Training loss. 0.2863786816596985 Num fake examples 84 Num true examples 76\n",
    "  Batch   120  of  53,679.    Elapsed: 0:00:05. Training loss. 1.4130690097808838 Num fake examples 123 Num true examples 117\n",
    "  Batch   160  of  53,679.    Elapsed: 0:00:06. Training loss. 0.2725940942764282 Num fake examples 161 Num true examples 159\n",
    "  Batch   200  of  53,679.    Elapsed: 0:00:08. Training loss. 0.05451549217104912 Num fake examples 203 Num true examples 197\n",
    "  Batch   240  of  53,679.    Elapsed: 0:00:09. Training loss. 0.020893290638923645 Num fake examples 238 Num true examples 242\n",
    "  Batch   280  of  53,679.    Elapsed: 0:00:11. Training loss. 0.017796985805034637 Num fake examples 268 Num true examples 292\n",
    "  Batch   320  of  53,679.    Elapsed: 0:00:12. Training loss. 0.014898774214088917 Num fake examples 312 Num true examples 328\n",
    "  Batch   360  of  53,679.    Elapsed: 0:00:13. Training loss. 0.016467485576868057 Num fake examples 351 Num true examples 369\n",
    "  Batch   400  of  53,679.    Elapsed: 0:00:15. Training loss. 1.6031320095062256 Num fake examples 380 Num true examples 420\n",
    "  Batch   440  of  53,679.    Elapsed: 0:00:16. Training loss. 0.008630537427961826 Num fake examples 415 Num true examples 465\n",
    "  Batch   480  of  53,679.    Elapsed: 0:00:18. Training loss. 0.029949575662612915 Num fake examples 457 Num true examples 503\n",
    "  Batch   520  of  53,679.    Elapsed: 0:00:20. Training loss. 0.011056065559387207 Num fake examples 491 Num true examples 549\n",
    "  Batch   560  of  53,679.    Elapsed: 0:00:21. Training loss. 2.5509636402130127 Num fake examples 527 Num true examples 593\n",
    "  Batch   600  of  53,679.    Elapsed: 0:00:23. Training loss. 0.008495982736349106 Num fake examples 562 Num true examples 638\n",
    "  Batch   640  of  53,679.    Elapsed: 0:00:24. Training loss. 2.457291603088379 Num fake examples 600 Num true examples 680\n",
    "  Batch   680  of  53,679.    Elapsed: 0:00:26. Training loss. 0.013291750103235245 Num fake examples 637 Num true examples 723\n",
    "  Batch   720  of  53,679.    Elapsed: 0:00:27. Training loss. 4.669824600219727 Num fake examples 679 Num true examples 761\n",
    "  Batch   760  of  53,679.    Elapsed: 0:00:29. Training loss. 1.618473768234253 Num fake examples 717 Num true examples 803\n",
    "  Batch   800  of  53,679.    Elapsed: 0:00:30. Training loss. 0.004787991754710674 Num fake examples 754 Num true examples 846\n",
    "  Batch   840  of  53,679.    Elapsed: 0:00:32. Training loss. 2.662468671798706 Num fake examples 790 Num true examples 890\n",
    "  Batch   880  of  53,679.    Elapsed: 0:00:33. Training loss. 0.008531550876796246 Num fake examples 827 Num true examples 933\n",
    "  Batch   920  of  53,679.    Elapsed: 0:00:35. Training loss. 0.0071554286405444145 Num fake examples 866 Num true examples 974\n",
    "  Batch   960  of  53,679.    Elapsed: 0:00:36. Training loss. 2.4785807132720947 Num fake examples 904 Num true examples 1016\n",
    "  Batch 1,000  of  53,679.    Elapsed: 0:00:38. Training loss. 0.006001788657158613 Num fake examples 940 Num true examples 1060\n",
    "  Batch 1,040  of  53,679.    Elapsed: 0:00:39. Training loss. 0.004775515757501125 Num fake examples 976 Num true examples 1104\n",
    "  Batch 1,080  of  53,679.    Elapsed: 0:00:41. Training loss. 2.5450472831726074 Num fake examples 1014 Num true examples 1146\n",
    "  Batch 1,120  of  53,679.    Elapsed: 0:00:42. Training loss. 0.005675249733030796 Num fake examples 1055 Num true examples 1185\n",
    "  Batch 1,160  of  53,679.    Elapsed: 0:00:44. Training loss. 0.004603246226906776 Num fake examples 1098 Num true examples 1222\n",
    "  Batch 1,200  of  53,679.    Elapsed: 0:00:45. Training loss. 0.009119685739278793 Num fake examples 1148 Num true examples 1252\n",
    "  Batch 1,240  of  53,679.    Elapsed: 0:00:47. Training loss. 0.005372404586523771 Num fake examples 1188 Num true examples 1292\n",
    "  Batch 1,280  of  53,679.    Elapsed: 0:00:48. Training loss. 0.0058461022563278675 Num fake examples 1224 Num true examples 1336\n",
    "  Batch 1,320  of  53,679.    Elapsed: 0:00:50. Training loss. 0.012630203738808632 Num fake examples 1264 Num true examples 1376\n",
    "  Batch 1,360  of  53,679.    Elapsed: 0:00:51. Training loss. 0.014851994812488556 Num fake examples 1300 Num true examples 1420\n",
    "  Batch 1,400  of  53,679.    Elapsed: 0:00:53. Training loss. 0.010972910560667515 Num fake examples 1344 Num true examples 1456\n",
    "  Batch 1,440  of  53,679.    Elapsed: 0:00:54. Training loss. 0.008869857527315617 Num fake examples 1378 Num true examples 1502\n",
    "  Batch 1,480  of  53,679.    Elapsed: 0:00:56. Training loss. 0.037235211580991745 Num fake examples 1417 Num true examples 1543\n",
    "  Batch 1,520  of  53,679.    Elapsed: 0:00:57. Training loss. 0.00962429866194725 Num fake examples 1458 Num true examples 1582\n",
    "  Batch 1,560  of  53,679.    Elapsed: 0:00:59. Training loss. 0.006878330372273922 Num fake examples 1496 Num true examples 1624\n",
    "  Batch 1,600  of  53,679.    Elapsed: 0:01:00. Training loss. 0.4513009190559387 Num fake examples 1533 Num true examples 1667\n",
    "  Batch 1,640  of  53,679.    Elapsed: 0:01:02. Training loss. 0.007722075562924147 Num fake examples 1568 Num true examples 1712\n",
    "  Batch 1,680  of  53,679.    Elapsed: 0:01:04. Training loss. 0.007105719763785601 Num fake examples 1604 Num true examples 1756\n",
    "  Batch 1,720  of  53,679.    Elapsed: 0:01:05. Training loss. 0.005591853521764278 Num fake examples 1634 Num true examples 1806\n",
    "  Batch 1,760  of  53,679.    Elapsed: 0:01:07. Training loss. 0.0069910213351249695 Num fake examples 1674 Num true examples 1846\n",
    "  Batch 1,800  of  53,679.    Elapsed: 0:01:08. Training loss. 0.005655865650624037 Num fake examples 1716 Num true examples 1884\n",
    "  Batch 1,840  of  53,679.    Elapsed: 0:01:10. Training loss. 0.012531334534287453 Num fake examples 1761 Num true examples 1919\n",
    "  Batch 1,880  of  53,679.    Elapsed: 0:01:12. Training loss. 0.004889672622084618 Num fake examples 1804 Num true examples 1956\n",
    "  Batch 1,920  of  53,679.    Elapsed: 0:01:13. Training loss. 0.0038010706193745136 Num fake examples 1843 Num true examples 1997\n",
    "  Batch 1,960  of  53,679.    Elapsed: 0:01:15. Training loss. 0.004305127076804638 Num fake examples 1877 Num true examples 2043\n",
    "  Batch 2,000  of  53,679.    Elapsed: 0:01:17. Training loss. 0.0038142073899507523 Num fake examples 1910 Num true examples 2090\n",
    "\n",
    "  Average training loss: 0.02\n",
    "  Training epcoh took: 0:01:17\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.14\n",
    "  Validation Loss: 0.05\n",
    "  Validation took: 0:00:22\n",
    "\n",
    "======== Epoch 2 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  53,679.    Elapsed: 0:00:02. Training loss. 0.0051093073561787605 Num fake examples 37 Num true examples 43\n",
    "  Batch    80  of  53,679.    Elapsed: 0:00:03. Training loss. 0.00870213471353054 Num fake examples 76 Num true examples 84\n",
    "  Batch   120  of  53,679.    Elapsed: 0:00:05. Training loss. 0.0034547820687294006 Num fake examples 117 Num true examples 123\n",
    "  Batch   160  of  53,679.    Elapsed: 0:00:06. Training loss. 0.005119015462696552 Num fake examples 155 Num true examples 165\n",
    "  Batch   200  of  53,679.    Elapsed: 0:00:08. Training loss. 0.004585428163409233 Num fake examples 204 Num true examples 196\n",
    "  Batch   240  of  53,679.    Elapsed: 0:00:09. Training loss. 0.00425070570781827 Num fake examples 242 Num true examples 238\n",
    "  Batch   280  of  53,679.    Elapsed: 0:00:11. Training loss. 2.1264727115631104 Num fake examples 284 Num true examples 276\n",
    "  Batch   320  of  53,679.    Elapsed: 0:00:12. Training loss. 0.006539365742355585 Num fake examples 319 Num true examples 321\n",
    "  Batch   360  of  53,679.    Elapsed: 0:00:14. Training loss. 0.011837886646389961 Num fake examples 352 Num true examples 368\n",
    "  Batch   400  of  53,679.    Elapsed: 0:00:15. Training loss. 0.009481767192482948 Num fake examples 385 Num true examples 415\n",
    "  Batch   440  of  53,679.    Elapsed: 0:00:17. Training loss. 0.006797300186008215 Num fake examples 430 Num true examples 450\n",
    "  Batch   480  of  53,679.    Elapsed: 0:00:18. Training loss. 0.011017213575541973 Num fake examples 474 Num true examples 486\n",
    "  Batch   520  of  53,679.    Elapsed: 0:00:20. Training loss. 0.008708981797099113 Num fake examples 512 Num true examples 528\n",
    "  Batch   560  of  53,679.    Elapsed: 0:00:21. Training loss. 0.007085148245096207 Num fake examples 556 Num true examples 564\n",
    "  Batch   600  of  53,679.    Elapsed: 0:00:23. Training loss. 0.008525745943188667 Num fake examples 591 Num true examples 609\n",
    "  Batch   640  of  53,679.    Elapsed: 0:00:24. Training loss. 0.010247371159493923 Num fake examples 628 Num true examples 652\n",
    "  Batch   680  of  53,679.    Elapsed: 0:00:26. Training loss. 0.010279848240315914 Num fake examples 664 Num true examples 696\n",
    "  Batch   720  of  53,679.    Elapsed: 0:00:27. Training loss. 2.589614152908325 Num fake examples 708 Num true examples 732\n",
    "  Batch   760  of  53,679.    Elapsed: 0:00:29. Training loss. 0.006221589632332325 Num fake examples 747 Num true examples 773\n",
    "  Batch   800  of  53,679.    Elapsed: 0:00:30. Training loss. 0.0070614018477499485 Num fake examples 782 Num true examples 818\n",
    "  Batch   840  of  53,679.    Elapsed: 0:00:32. Training loss. 0.004075550474226475 Num fake examples 826 Num true examples 854\n",
    "  Batch   880  of  53,679.    Elapsed: 0:00:33. Training loss. 0.0065109943971037865 Num fake examples 868 Num true examples 892\n",
    "  Batch   920  of  53,679.    Elapsed: 0:00:35. Training loss. 0.006801750510931015 Num fake examples 908 Num true examples 932\n",
    "  Batch   960  of  53,679.    Elapsed: 0:00:36. Training loss. 0.005712343379855156 Num fake examples 958 Num true examples 962\n",
    "  Batch 1,000  of  53,679.    Elapsed: 0:00:38. Training loss. 2.5652239322662354 Num fake examples 986 Num true examples 1014\n",
    "  Batch 1,040  of  53,679.    Elapsed: 0:00:39. Training loss. 0.008335079997777939 Num fake examples 1028 Num true examples 1052\n",
    "  Batch 1,080  of  53,679.    Elapsed: 0:00:41. Training loss. 0.006399214733392 Num fake examples 1069 Num true examples 1091\n",
    "  Batch 1,120  of  53,679.    Elapsed: 0:00:42. Training loss. 0.01047724299132824 Num fake examples 1111 Num true examples 1129\n",
    "  Batch 1,160  of  53,679.    Elapsed: 0:00:44. Training loss. 0.005419911816716194 Num fake examples 1151 Num true examples 1169\n",
    "  Batch 1,200  of  53,679.    Elapsed: 0:00:45. Training loss. 0.004511283710598946 Num fake examples 1196 Num true examples 1204\n",
    "  Batch 1,240  of  53,679.    Elapsed: 0:00:47. Training loss. 0.005863734520971775 Num fake examples 1239 Num true examples 1241\n",
    "  Batch 1,280  of  53,679.    Elapsed: 0:00:48. Training loss. 0.004446594510227442 Num fake examples 1278 Num true examples 1282\n",
    "  Batch 1,320  of  53,679.    Elapsed: 0:00:50. Training loss. 0.004088265355676413 Num fake examples 1312 Num true examples 1328\n",
    "  Batch 1,360  of  53,679.    Elapsed: 0:00:51. Training loss. 0.005931519903242588 Num fake examples 1354 Num true examples 1366\n",
    "  Batch 1,400  of  53,679.    Elapsed: 0:00:53. Training loss. 0.005395882297307253 Num fake examples 1395 Num true examples 1405\n",
    "  Batch 1,440  of  53,679.    Elapsed: 0:00:54. Training loss. 0.0051335301250219345 Num fake examples 1428 Num true examples 1452\n",
    "  Batch 1,480  of  53,679.    Elapsed: 0:00:56. Training loss. 0.0057206954807043076 Num fake examples 1462 Num true examples 1498\n",
    "  Batch 1,520  of  53,679.    Elapsed: 0:00:57. Training loss. 0.006923328619450331 Num fake examples 1493 Num true examples 1547\n",
    "  Batch 1,560  of  53,679.    Elapsed: 0:00:59. Training loss. 0.00486648129299283 Num fake examples 1533 Num true examples 1587\n",
    "  Batch 1,600  of  53,679.    Elapsed: 0:01:00. Training loss. 0.0048270272091031075 Num fake examples 1566 Num true examples 1634\n",
    "  Batch 1,640  of  53,679.    Elapsed: 0:01:02. Training loss. 0.00492424052208662 Num fake examples 1606 Num true examples 1674\n",
    "  Batch 1,680  of  53,679.    Elapsed: 0:01:03. Training loss. 0.005969040095806122 Num fake examples 1652 Num true examples 1708\n",
    "  Batch 1,720  of  53,679.    Elapsed: 0:01:05. Training loss. 0.005786978639662266 Num fake examples 1692 Num true examples 1748\n",
    "  Batch 1,760  of  53,679.    Elapsed: 0:01:06. Training loss. 0.005638184025883675 Num fake examples 1724 Num true examples 1796\n",
    "  Batch 1,800  of  53,679.    Elapsed: 0:01:08. Training loss. 0.004907557740807533 Num fake examples 1766 Num true examples 1834\n",
    "  Batch 1,840  of  53,679.    Elapsed: 0:01:09. Training loss. 0.006175585091114044 Num fake examples 1795 Num true examples 1885\n",
    "  Batch 1,880  of  53,679.    Elapsed: 0:01:11. Training loss. 0.008886728435754776 Num fake examples 1832 Num true examples 1928\n",
    "  Batch 1,920  of  53,679.    Elapsed: 0:01:12. Training loss. 0.011807069182395935 Num fake examples 1874 Num true examples 1966\n",
    "  Batch 1,960  of  53,679.    Elapsed: 0:01:14. Training loss. 0.006916789337992668 Num fake examples 1911 Num true examples 2009\n",
    "  Batch 2,000  of  53,679.    Elapsed: 0:01:15. Training loss. 0.006858722306787968 Num fake examples 1946 Num true examples 2054\n",
    "\n",
    "  Average training loss: 0.01\n",
    "  Training epcoh took: 0:01:15\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.14\n",
    "  Validation Loss: 0.04\n",
    "  Validation took: 0:00:20\n",
    "\n",
    "======== Epoch 3 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  53,679.    Elapsed: 0:00:02. Training loss. 0.004842684138566256 Num fake examples 40 Num true examples 40\n",
    "  Batch    80  of  53,679.    Elapsed: 0:00:03. Training loss. 0.005506762303411961 Num fake examples 81 Num true examples 79\n",
    "  Batch   120  of  53,679.    Elapsed: 0:00:05. Training loss. 5.256283760070801 Num fake examples 119 Num true examples 121\n",
    "  Batch   160  of  53,679.    Elapsed: 0:00:06. Training loss. 0.01021522469818592 Num fake examples 160 Num true examples 160\n",
    "  Batch   200  of  53,679.    Elapsed: 0:00:08. Training loss. 0.00931387860327959 Num fake examples 204 Num true examples 196\n",
    "  Batch   240  of  53,679.    Elapsed: 0:00:09. Training loss. 0.005680035799741745 Num fake examples 239 Num true examples 241\n",
    "  Batch   280  of  53,679.    Elapsed: 0:00:11. Training loss. 0.015501441434025764 Num fake examples 277 Num true examples 283\n",
    "  Batch   320  of  53,679.    Elapsed: 0:00:12. Training loss. 0.003119553904980421 Num fake examples 318 Num true examples 322\n",
    "  Batch   360  of  53,679.    Elapsed: 0:00:14. Training loss. 0.004763524979352951 Num fake examples 359 Num true examples 361\n",
    "  Batch   400  of  53,679.    Elapsed: 0:00:15. Training loss. 0.004203564487397671 Num fake examples 394 Num true examples 406\n",
    "  Batch   440  of  53,679.    Elapsed: 0:00:17. Training loss. 0.0027705999091267586 Num fake examples 430 Num true examples 450\n",
    "  Batch   480  of  53,679.    Elapsed: 0:00:18. Training loss. 0.003083182265982032 Num fake examples 466 Num true examples 494\n",
    "  Batch   520  of  53,679.    Elapsed: 0:00:20. Training loss. 0.0026189428754150867 Num fake examples 511 Num true examples 529\n",
    "  Batch   560  of  53,679.    Elapsed: 0:00:21. Training loss. 0.006949164904654026 Num fake examples 545 Num true examples 575\n",
    "  Batch   600  of  53,679.    Elapsed: 0:00:23. Training loss. 0.0058027636259794235 Num fake examples 583 Num true examples 617\n",
    "  Batch   640  of  53,679.    Elapsed: 0:00:24. Training loss. 2.652974843978882 Num fake examples 617 Num true examples 663\n",
    "  Batch   680  of  53,679.    Elapsed: 0:00:26. Training loss. 0.007209605537354946 Num fake examples 660 Num true examples 700\n",
    "  Batch   720  of  53,679.    Elapsed: 0:00:27. Training loss. 0.009214762598276138 Num fake examples 692 Num true examples 748\n",
    "  Batch   760  of  53,679.    Elapsed: 0:00:29. Training loss. 0.0053913891315460205 Num fake examples 731 Num true examples 789\n",
    "  Batch   800  of  53,679.    Elapsed: 0:00:30. Training loss. 0.0055155945010483265 Num fake examples 771 Num true examples 829\n",
    "  Batch   840  of  53,679.    Elapsed: 0:00:32. Training loss. 0.005464968271553516 Num fake examples 810 Num true examples 870\n",
    "  Batch   880  of  53,679.    Elapsed: 0:00:33. Training loss. 0.006212316919118166 Num fake examples 850 Num true examples 910\n",
    "  Batch   920  of  53,679.    Elapsed: 0:00:35. Training loss. 0.0038482414092868567 Num fake examples 892 Num true examples 948\n",
    "  Batch   960  of  53,679.    Elapsed: 0:00:36. Training loss. 0.006037633866071701 Num fake examples 931 Num true examples 989\n",
    "  Batch 1,000  of  53,679.    Elapsed: 0:00:38. Training loss. 0.00693658459931612 Num fake examples 969 Num true examples 1031\n",
    "  Batch 1,040  of  53,679.    Elapsed: 0:00:39. Training loss. 0.004665445536375046 Num fake examples 1003 Num true examples 1077\n",
    "  Batch 1,080  of  53,679.    Elapsed: 0:00:41. Training loss. 0.0037997302133589983 Num fake examples 1045 Num true examples 1115\n",
    "  Batch 1,120  of  53,679.    Elapsed: 0:00:42. Training loss. 0.004203146323561668 Num fake examples 1081 Num true examples 1159\n",
    "  Batch 1,160  of  53,679.    Elapsed: 0:00:44. Training loss. 0.00476829195395112 Num fake examples 1118 Num true examples 1202\n",
    "  Batch 1,200  of  53,679.    Elapsed: 0:00:45. Training loss. 0.11998821794986725 Num fake examples 1155 Num true examples 1245\n",
    "  Batch 1,240  of  53,679.    Elapsed: 0:00:47. Training loss. 0.006978004705160856 Num fake examples 1199 Num true examples 1281\n",
    "  Batch 1,280  of  53,679.    Elapsed: 0:00:48. Training loss. 0.005160256754606962 Num fake examples 1241 Num true examples 1319\n",
    "  Batch 1,320  of  53,679.    Elapsed: 0:00:50. Training loss. 0.005140017718076706 Num fake examples 1271 Num true examples 1369\n",
    "  Batch 1,360  of  53,679.    Elapsed: 0:00:51. Training loss. 0.008780686184763908 Num fake examples 1322 Num true examples 1398\n",
    "  Batch 1,400  of  53,679.    Elapsed: 0:00:53. Training loss. 0.007382241543382406 Num fake examples 1355 Num true examples 1445\n",
    "  Batch 1,440  of  53,679.    Elapsed: 0:00:54. Training loss. 0.0055897170677781105 Num fake examples 1393 Num true examples 1487\n",
    "  Batch 1,480  of  53,679.    Elapsed: 0:00:56. Training loss. 0.006115456111729145 Num fake examples 1432 Num true examples 1528\n",
    "  Batch 1,520  of  53,679.    Elapsed: 0:00:57. Training loss. 0.031881194561719894 Num fake examples 1465 Num true examples 1575\n",
    "  Batch 1,560  of  53,679.    Elapsed: 0:00:59. Training loss. 0.007711710408329964 Num fake examples 1508 Num true examples 1612\n",
    "  Batch 1,600  of  53,679.    Elapsed: 0:01:00. Training loss. 0.005309962201863527 Num fake examples 1546 Num true examples 1654\n",
    "  Batch 1,640  of  53,679.    Elapsed: 0:01:02. Training loss. 0.0047393860295414925 Num fake examples 1579 Num true examples 1701\n",
    "  Batch 1,680  of  53,679.    Elapsed: 0:01:03. Training loss. 0.005430717021226883 Num fake examples 1617 Num true examples 1743\n",
    "  Batch 1,720  of  53,679.    Elapsed: 0:01:05. Training loss. 0.0033990012016147375 Num fake examples 1649 Num true examples 1791\n",
    "  Batch 1,760  of  53,679.    Elapsed: 0:01:06. Training loss. 2.1168272495269775 Num fake examples 1682 Num true examples 1838\n",
    "  Batch 1,800  of  53,679.    Elapsed: 0:01:08. Training loss. 0.005855056457221508 Num fake examples 1713 Num true examples 1887\n",
    "  Batch 1,840  of  53,679.    Elapsed: 0:01:09. Training loss. 0.0033687585964798927 Num fake examples 1752 Num true examples 1928\n",
    "  Batch 1,880  of  53,679.    Elapsed: 0:01:11. Training loss. 0.00365239754319191 Num fake examples 1790 Num true examples 1970\n",
    "  Batch 1,920  of  53,679.    Elapsed: 0:01:12. Training loss. 0.005667196586728096 Num fake examples 1823 Num true examples 2017\n",
    "  Batch 1,960  of  53,679.    Elapsed: 0:01:14. Training loss. 0.005332340020686388 Num fake examples 1859 Num true examples 2061\n",
    "  Batch 2,000  of  53,679.    Elapsed: 0:01:15. Training loss. 0.005762225948274136 Num fake examples 1897 Num true examples 2103\n",
    "\n",
    "  Average training loss: 0.01\n",
    "  Training epcoh took: 0:01:15\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.14\n",
    "  Validation Loss: 0.04\n",
    "  Validation took: 0:00:21\n",
    "\n",
    "======== Epoch 4 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  53,679.    Elapsed: 0:00:01. Training loss. 0.01007026992738247 Num fake examples 40 Num true examples 40\n",
    "  Batch    80  of  53,679.    Elapsed: 0:00:03. Training loss. 0.0058093625120818615 Num fake examples 75 Num true examples 85\n",
    "  Batch   120  of  53,679.    Elapsed: 0:00:05. Training loss. 0.006527090445160866 Num fake examples 116 Num true examples 124\n",
    "  Batch   160  of  53,679.    Elapsed: 0:00:06. Training loss. 0.007764199748635292 Num fake examples 148 Num true examples 172\n",
    "  Batch   200  of  53,679.    Elapsed: 0:00:08. Training loss. 0.006081189028918743 Num fake examples 189 Num true examples 211\n",
    "  Batch   240  of  53,679.    Elapsed: 0:00:09. Training loss. 0.007699153386056423 Num fake examples 225 Num true examples 255\n",
    "  Batch   280  of  53,679.    Elapsed: 0:00:11. Training loss. 0.004653744399547577 Num fake examples 267 Num true examples 293\n",
    "  Batch   320  of  53,679.    Elapsed: 0:00:12. Training loss. 0.0037357639521360397 Num fake examples 310 Num true examples 330\n",
    "  Batch   360  of  53,679.    Elapsed: 0:00:14. Training loss. 2.8481810092926025 Num fake examples 348 Num true examples 372\n",
    "  Batch   400  of  53,679.    Elapsed: 0:00:15. Training loss. 0.004053334705531597 Num fake examples 387 Num true examples 413\n",
    "  Batch   440  of  53,679.    Elapsed: 0:00:17. Training loss. 0.0054313139989972115 Num fake examples 429 Num true examples 451\n",
    "  Batch   480  of  53,679.    Elapsed: 0:00:18. Training loss. 0.012490440160036087 Num fake examples 464 Num true examples 496\n",
    "  Batch   520  of  53,679.    Elapsed: 0:00:20. Training loss. 0.012847645208239555 Num fake examples 501 Num true examples 539\n",
    "  Batch   560  of  53,679.    Elapsed: 0:00:21. Training loss. 0.008191321045160294 Num fake examples 529 Num true examples 591\n",
    "  Batch   600  of  53,679.    Elapsed: 0:00:23. Training loss. 0.00598276499658823 Num fake examples 564 Num true examples 636\n",
    "  Batch   640  of  53,679.    Elapsed: 0:00:24. Training loss. 0.007833411917090416 Num fake examples 607 Num true examples 673\n",
    "  Batch   680  of  53,679.    Elapsed: 0:00:26. Training loss. 0.00454895943403244 Num fake examples 641 Num true examples 719\n",
    "  Batch   720  of  53,679.    Elapsed: 0:00:27. Training loss. 0.0053139664232730865 Num fake examples 678 Num true examples 762\n",
    "  Batch   760  of  53,679.    Elapsed: 0:00:29. Training loss. 0.006552906706929207 Num fake examples 719 Num true examples 801\n",
    "  Batch   800  of  53,679.    Elapsed: 0:00:30. Training loss. 0.0069026341661810875 Num fake examples 761 Num true examples 839\n",
    "  Batch   840  of  53,679.    Elapsed: 0:00:32. Training loss. 0.0062430789694190025 Num fake examples 801 Num true examples 879\n",
    "  Batch   880  of  53,679.    Elapsed: 0:00:33. Training loss. 0.006081201136112213 Num fake examples 842 Num true examples 918\n",
    "  Batch   920  of  53,679.    Elapsed: 0:00:35. Training loss. 0.005057710688561201 Num fake examples 883 Num true examples 957\n",
    "  Batch   960  of  53,679.    Elapsed: 0:00:36. Training loss. 0.003937508910894394 Num fake examples 920 Num true examples 1000\n",
    "  Batch 1,000  of  53,679.    Elapsed: 0:00:38. Training loss. 0.003811266738921404 Num fake examples 957 Num true examples 1043\n",
    "  Batch 1,040  of  53,679.    Elapsed: 0:00:39. Training loss. 0.0025482436176389456 Num fake examples 996 Num true examples 1084\n",
    "  Batch 1,080  of  53,679.    Elapsed: 0:00:41. Training loss. 0.003340918803587556 Num fake examples 1038 Num true examples 1122\n",
    "  Batch 1,120  of  53,679.    Elapsed: 0:00:42. Training loss. 0.004712732508778572 Num fake examples 1070 Num true examples 1170\n",
    "  Batch 1,160  of  53,679.    Elapsed: 0:00:44. Training loss. 0.006288493052124977 Num fake examples 1116 Num true examples 1204\n",
    "  Batch 1,200  of  53,679.    Elapsed: 0:00:45. Training loss. 2.562509059906006 Num fake examples 1149 Num true examples 1251\n",
    "  Batch 1,240  of  53,679.    Elapsed: 0:00:47. Training loss. 0.007800639607012272 Num fake examples 1186 Num true examples 1294\n",
    "  Batch 1,280  of  53,679.    Elapsed: 0:00:48. Training loss. 0.005326135084033012 Num fake examples 1224 Num true examples 1336\n",
    "  Batch 1,320  of  53,679.    Elapsed: 0:00:50. Training loss. 0.005355541594326496 Num fake examples 1264 Num true examples 1376\n",
    "  Batch 1,360  of  53,679.    Elapsed: 0:00:51. Training loss. 0.006113925948739052 Num fake examples 1308 Num true examples 1412\n",
    "  Batch 1,400  of  53,679.    Elapsed: 0:00:53. Training loss. 0.006181278266012669 Num fake examples 1347 Num true examples 1453\n",
    "  Batch 1,440  of  53,679.    Elapsed: 0:00:54. Training loss. 0.006779635325074196 Num fake examples 1385 Num true examples 1495\n",
    "  Batch 1,480  of  53,679.    Elapsed: 0:00:56. Training loss. 0.17459948360919952 Num fake examples 1426 Num true examples 1534\n",
    "  Batch 1,520  of  53,679.    Elapsed: 0:00:57. Training loss. 4.935765266418457 Num fake examples 1464 Num true examples 1576\n",
    "  Batch 1,560  of  53,679.    Elapsed: 0:00:59. Training loss. 0.0068906270898878574 Num fake examples 1505 Num true examples 1615\n",
    "  Batch 1,600  of  53,679.    Elapsed: 0:01:00. Training loss. 0.008055232465267181 Num fake examples 1544 Num true examples 1656\n",
    "  Batch 1,640  of  53,679.    Elapsed: 0:01:02. Training loss. 0.011395520530641079 Num fake examples 1575 Num true examples 1705\n",
    "  Batch 1,680  of  53,679.    Elapsed: 0:01:04. Training loss. 0.005793886259198189 Num fake examples 1616 Num true examples 1744\n",
    "  Batch 1,720  of  53,679.    Elapsed: 0:01:05. Training loss. 0.0052419379353523254 Num fake examples 1659 Num true examples 1781\n",
    "  Batch 1,760  of  53,679.    Elapsed: 0:01:06. Training loss. 0.004910165909677744 Num fake examples 1699 Num true examples 1821\n",
    "  Batch 1,800  of  53,679.    Elapsed: 0:01:08. Training loss. 0.007167006377130747 Num fake examples 1739 Num true examples 1861\n",
    "  Batch 1,840  of  53,679.    Elapsed: 0:01:10. Training loss. 0.004657607525587082 Num fake examples 1782 Num true examples 1898\n",
    "  Batch 1,880  of  53,679.    Elapsed: 0:01:11. Training loss. 0.003261711448431015 Num fake examples 1823 Num true examples 1937\n",
    "  Batch 1,920  of  53,679.    Elapsed: 0:01:13. Training loss. 0.005801088642328978 Num fake examples 1865 Num true examples 1975\n",
    "  Batch 1,960  of  53,679.    Elapsed: 0:01:14. Training loss. 0.004804220981895924 Num fake examples 1888 Num true examples 2032\n",
    "  Batch 2,000  of  53,679.    Elapsed: 0:01:16. Training loss. 0.005264644511044025 Num fake examples 1932 Num true examples 2068\n",
    "\n",
    "  Average training loss: 0.01\n",
    "  Training epcoh took: 0:01:16\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.14\n",
    "  Validation Loss: 0.06\n",
    "  Validation took: 0:00:20\n",
    "\n",
    "Training complete!\n",
    "Total training took 0:06:28 (h:mm:ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a40c1-19b7-4ebe-98c8-c3b31b8cc10b",
   "metadata": {},
   "source": [
    "## Original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa2821-82b2-4bad-a39b-dbda2fac32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d1c5f-724d-4255-ba08-7328c9b5c5e8",
   "metadata": {},
   "source": [
    "## UPDATE\n",
    "Batch Size of 32: Efficiently balances memory usage and convergence speed.\n",
    "Random Sampler for Training: Ensures the model generalizes better by shuffling data every epoch.\n",
    "Sequential Sampler for Validation: Avoids unnecessary shuffling in validation, keeping results stable and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37851897-5c52-4dc3-8ada-7517385309ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,                   # The training dataset\n",
    "    sampler=RandomSampler(train_dataset),  # Random sampling for shuffling\n",
    "    batch_size=batch_size            # Number of samples per batch\n",
    ")\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,                     # The validation dataset\n",
    "    sampler=SequentialSampler(val_dataset),  # Sequential sampling for consistent evaluation\n",
    "    batch_size=batch_size            # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15800d-7161-4f81-b249-2ac6a0aab60c",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdd948-ab86-4beb-9d07-eed8f7e86c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acc177-2118-4a28-9844-c0b20c9df018",
   "metadata": {},
   "source": [
    "## UPDATE\n",
    "Comparison: With vs Without Warmup\n",
    "Without Warmup:\n",
    "\n",
    "Learning rate starts at the target value (e.g., 2e-5).\n",
    "Early steps may destabilize training, leading to slower convergence or suboptimal results.\n",
    "With Warmup:\n",
    "\n",
    "Learning rate gradually increases during the warmup phase, making the initial updates more stable and effective.\n",
    "Allows the model to adjust to the task-specific dataset more gracefully.\n",
    "When to Use Warmup Steps\n",
    "Always Recommended:\n",
    "When fine-tuning pre-trained models like BERT or other transformers, warmup steps are considered best practice.\n",
    "10% Warmup Steps:\n",
    "Common choice for num_warmup_steps is 10% of total_steps, as used in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef35520-e2e8-42da-9188-6b86d1d7f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce52bd-308c-4f6d-a520-e3f395ba7e78",
   "metadata": {},
   "source": [
    " # Original \n",
    " For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44578955-35d6-4725-bb11-3f3a33ecd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1ec69-128a-4020-8013-0ead2c85271c",
   "metadata": {},
   "source": [
    "# UPDATE\n",
    "More Complete Training:\n",
    "\n",
    "Increasing the number of steps allows the model to train on a larger portion of the dataset, improving its ability to generalize to unseen data.\n",
    "Better Representation of Dataset:\n",
    "\n",
    "With only 2000 steps, the model may not see enough of the dataset's variability, especially for imbalanced or diverse datasets. 8000 steps give it a more representative sample of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cefc0-16d9-47b4-9e7c-639af9ac9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    " # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 8000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f245f-8243-49c7-ba4f-676e393fcf46",
   "metadata": {},
   "source": [
    " # Original \n",
    " For validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3e242-1a28-48a2-9960-ccecd626e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    " # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ba70f-641e-4d10-a295-1dba601744b8",
   "metadata": {},
   "source": [
    "# UPDATE\n",
    "More Complete Training:\n",
    "\n",
    "Increasing the number of steps allows the model to train on a larger portion of the dataset, improving its ability to generalize to unseen data.\n",
    "Better Representation of Dataset:\n",
    "\n",
    "With only 2000 steps, the model may not see enough of the dataset's variability, especially for imbalanced or diverse datasets. 8000 steps give it a more representative sample of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebb511-e9c3-49d4-b165-b3f44e79cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    " # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 8000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7bb6e-f959-4c9a-bef5-a4b6d86a8e3a",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc664f14-6613-4702-95bc-8bf19f96aa21",
   "metadata": {},
   "source": [
    "```\n",
    "======== Epoch 1 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  3,355.    Elapsed: 0:00:13. Training loss. 0.05325314402580261 Num fake examples 637 Num true examples 643\n",
    "  Batch    80  of  3,355.    Elapsed: 0:00:27. Training loss. 0.04623487591743469 Num fake examples 1263 Num true examples 1297\n",
    "  Batch   120  of  3,355.    Elapsed: 0:00:41. Training loss. 0.045033201575279236 Num fake examples 1903 Num true examples 1937\n",
    "  Batch   160  of  3,355.    Elapsed: 0:00:56. Training loss. 0.34403926134109497 Num fake examples 2472 Num true examples 2648\n",
    "  Batch   200  of  3,355.    Elapsed: 0:01:10. Training loss. 0.4330782890319824 Num fake examples 3089 Num true examples 3311\n",
    "  Batch   240  of  3,355.    Elapsed: 0:01:25. Training loss. 0.1362992227077484 Num fake examples 3702 Num true examples 3978\n",
    "  Batch   280  of  3,355.    Elapsed: 0:01:39. Training loss. 0.34318041801452637 Num fake examples 4338 Num true examples 4622\n",
    "  Batch   320  of  3,355.    Elapsed: 0:01:54. Training loss. 0.44923385977745056 Num fake examples 4973 Num true examples 5267\n",
    "  Batch   360  of  3,355.    Elapsed: 0:02:08. Training loss. 0.13802054524421692 Num fake examples 5574 Num true examples 5946\n",
    "  Batch   400  of  3,355.    Elapsed: 0:02:22. Training loss. 0.03904082626104355 Num fake examples 6211 Num true examples 6589\n",
    "  Batch   440  of  3,355.    Elapsed: 0:02:37. Training loss. 0.13255545496940613 Num fake examples 6820 Num true examples 7260\n",
    "  Batch   480  of  3,355.    Elapsed: 0:02:51. Training loss. 0.2304016500711441 Num fake examples 7445 Num true examples 7915\n",
    "  Batch   520  of  3,355.    Elapsed: 0:03:05. Training loss. 0.24732643365859985 Num fake examples 8056 Num true examples 8584\n",
    "  Batch   560  of  3,355.    Elapsed: 0:03:20. Training loss. 0.21916227042675018 Num fake examples 8639 Num true examples 9281\n",
    "  Batch   600  of  3,355.    Elapsed: 0:03:34. Training loss. 0.2414417713880539 Num fake examples 9242 Num true examples 9958\n",
    "  Batch   640  of  3,355.    Elapsed: 0:03:48. Training loss. 0.2489798665046692 Num fake examples 9899 Num true examples 10581\n",
    "  Batch   680  of  3,355.    Elapsed: 0:04:03. Training loss. 0.2443133145570755 Num fake examples 10529 Num true examples 11231\n",
    "  Batch   720  of  3,355.    Elapsed: 0:04:17. Training loss. 0.348847359418869 Num fake examples 11148 Num true examples 11892\n",
    "  Batch   760  of  3,355.    Elapsed: 0:04:32. Training loss. 0.03675999864935875 Num fake examples 11787 Num true examples 12533\n",
    "  Batch   800  of  3,355.    Elapsed: 0:04:46. Training loss. 0.14411288499832153 Num fake examples 12404 Num true examples 13196\n",
    "  Batch   840  of  3,355.    Elapsed: 0:05:00. Training loss. 0.35191139578819275 Num fake examples 13059 Num true examples 13821\n",
    "  Batch   880  of  3,355.    Elapsed: 0:05:15. Training loss. 0.35155338048934937 Num fake examples 13680 Num true examples 14480\n",
    "  Batch   920  of  3,355.    Elapsed: 0:05:29. Training loss. 0.14330990612506866 Num fake examples 14323 Num true examples 15117\n",
    "  Batch   960  of  3,355.    Elapsed: 0:05:43. Training loss. 0.1385488510131836 Num fake examples 14949 Num true examples 15771\n",
    "  Batch 1,000  of  3,355.    Elapsed: 0:05:58. Training loss. 0.14380037784576416 Num fake examples 15601 Num true examples 16399\n",
    "  Batch 1,040  of  3,355.    Elapsed: 0:06:12. Training loss. 0.22394728660583496 Num fake examples 16240 Num true examples 17040\n",
    "  Batch 1,080  of  3,355.    Elapsed: 0:06:27. Training loss. 0.03967425599694252 Num fake examples 16844 Num true examples 17716\n",
    "  Batch 1,120  of  3,355.    Elapsed: 0:06:41. Training loss. 0.23245097696781158 Num fake examples 17458 Num true examples 18382\n",
    "  Batch 1,160  of  3,355.    Elapsed: 0:06:55. Training loss. 0.03981921821832657 Num fake examples 18072 Num true examples 19048\n",
    "  Batch 1,200  of  3,355.    Elapsed: 0:07:10. Training loss. 0.15270698070526123 Num fake examples 18691 Num true examples 19709\n",
    "  Batch 1,240  of  3,355.    Elapsed: 0:07:24. Training loss. 0.13438370823860168 Num fake examples 19310 Num true examples 20370\n",
    "  Batch 1,280  of  3,355.    Elapsed: 0:07:39. Training loss. 0.12269096076488495 Num fake examples 19914 Num true examples 21046\n",
    "  Batch 1,320  of  3,355.    Elapsed: 0:07:53. Training loss. 0.13771042227745056 Num fake examples 20552 Num true examples 21688\n",
    "  Batch 1,360  of  3,355.    Elapsed: 0:08:07. Training loss. 0.14431773126125336 Num fake examples 21174 Num true examples 22346\n",
    "  Batch 1,400  of  3,355.    Elapsed: 0:08:22. Training loss. 0.1411183774471283 Num fake examples 21812 Num true examples 22988\n",
    "  Batch 1,440  of  3,355.    Elapsed: 0:08:36. Training loss. 0.1309300810098648 Num fake examples 22442 Num true examples 23638\n",
    "  Batch 1,480  of  3,355.    Elapsed: 0:08:50. Training loss. 0.04416937008500099 Num fake examples 23043 Num true examples 24317\n",
    "  Batch 1,520  of  3,355.    Elapsed: 0:09:05. Training loss. 0.05153686925768852 Num fake examples 23674 Num true examples 24966\n",
    "  Batch 1,560  of  3,355.    Elapsed: 0:09:19. Training loss. 0.24323059618473053 Num fake examples 24292 Num true examples 25628\n",
    "  Batch 1,600  of  3,355.    Elapsed: 0:09:34. Training loss. 0.1415989100933075 Num fake examples 24923 Num true examples 26277\n",
    "  Batch 1,640  of  3,355.    Elapsed: 0:09:48. Training loss. 0.14078573882579803 Num fake examples 25553 Num true examples 26927\n",
    "  Batch 1,680  of  3,355.    Elapsed: 0:10:03. Training loss. 0.03978294879198074 Num fake examples 26156 Num true examples 27604\n",
    "  Batch 1,720  of  3,355.    Elapsed: 0:10:17. Training loss. 0.1557288020849228 Num fake examples 26800 Num true examples 28240\n",
    "  Batch 1,760  of  3,355.    Elapsed: 0:10:31. Training loss. 0.14152830839157104 Num fake examples 27432 Num true examples 28888\n",
    "  Batch 1,800  of  3,355.    Elapsed: 0:10:46. Training loss. 0.2352922558784485 Num fake examples 28064 Num true examples 29536\n",
    "  Batch 1,840  of  3,355.    Elapsed: 0:11:00. Training loss. 0.14514034986495972 Num fake examples 28714 Num true examples 30166\n",
    "  Batch 1,880  of  3,355.    Elapsed: 0:11:15. Training loss. 0.13693968951702118 Num fake examples 29328 Num true examples 30832\n",
    "  Batch 1,920  of  3,355.    Elapsed: 0:11:29. Training loss. 0.3216586709022522 Num fake examples 29944 Num true examples 31496\n",
    "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.15134760737419128 Num fake examples 30549 Num true examples 32171\n",
    "  Batch 2,000  of  3,355.    Elapsed: 0:11:58. Training loss. 0.42317187786102295 Num fake examples 31184 Num true examples 32816\n",
    "  Batch 2,040  of  3,355.    Elapsed: 0:12:12. Training loss. 0.25259503722190857 Num fake examples 31794 Num true examples 33486\n",
    "  Batch 2,080  of  3,355.    Elapsed: 0:12:27. Training loss. 0.21366265416145325 Num fake examples 32395 Num true examples 34165\n",
    "  Batch 2,120  of  3,355.    Elapsed: 0:12:41. Training loss. 0.4712429642677307 Num fake examples 32983 Num true examples 34857\n",
    "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.10885500907897949 Num fake examples 33641 Num true examples 35479\n",
    "  Batch 2,200  of  3,355.    Elapsed: 0:13:10. Training loss. 0.047932203859090805 Num fake examples 34282 Num true examples 36118\n",
    "  Batch 2,240  of  3,355.    Elapsed: 0:13:24. Training loss. 0.2473597377538681 Num fake examples 34916 Num true examples 36764\n",
    "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.2504013776779175 Num fake examples 35530 Num true examples 37430\n",
    "  Batch 2,320  of  3,355.    Elapsed: 0:13:53. Training loss. 0.4930192232131958 Num fake examples 36141 Num true examples 38099\n",
    "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.049354881048202515 Num fake examples 36755 Num true examples 38765\n",
    "  Batch 2,400  of  3,355.    Elapsed: 0:14:22. Training loss. 0.2073425054550171 Num fake examples 37349 Num true examples 39451\n",
    "  Batch 2,440  of  3,355.    Elapsed: 0:14:36. Training loss. 0.14362500607967377 Num fake examples 37969 Num true examples 40111\n",
    "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.23683255910873413 Num fake examples 38588 Num true examples 40772\n",
    "  Batch 2,520  of  3,355.    Elapsed: 0:15:05. Training loss. 0.04684709012508392 Num fake examples 39238 Num true examples 41402\n",
    "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.1246090903878212 Num fake examples 39872 Num true examples 42048\n",
    "  Batch 2,600  of  3,355.    Elapsed: 0:15:34. Training loss. 0.20864659547805786 Num fake examples 40468 Num true examples 42732\n",
    "  Batch 2,640  of  3,355.    Elapsed: 0:15:48. Training loss. 0.14996300637722015 Num fake examples 41097 Num true examples 43383\n",
    "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.2482253462076187 Num fake examples 41709 Num true examples 44051\n",
    "  Batch 2,720  of  3,355.    Elapsed: 0:16:17. Training loss. 0.20617693662643433 Num fake examples 42336 Num true examples 44704\n",
    "  Batch 2,760  of  3,355.    Elapsed: 0:16:31. Training loss. 0.21365739405155182 Num fake examples 42958 Num true examples 45362\n",
    "  Batch 2,800  of  3,355.    Elapsed: 0:16:46. Training loss. 0.03816793113946915 Num fake examples 43581 Num true examples 46019\n",
    "  Batch 2,840  of  3,355.    Elapsed: 0:17:00. Training loss. 0.21265727281570435 Num fake examples 44194 Num true examples 46686\n",
    "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.13820533454418182 Num fake examples 44833 Num true examples 47327\n",
    "  Batch 2,920  of  3,355.    Elapsed: 0:17:29. Training loss. 0.25221556425094604 Num fake examples 45428 Num true examples 48012\n",
    "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.14562320709228516 Num fake examples 46051 Num true examples 48669\n",
    "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.04834777116775513 Num fake examples 46672 Num true examples 49328\n",
    "  Batch 3,040  of  3,355.    Elapsed: 0:18:12. Training loss. 0.1346035897731781 Num fake examples 47283 Num true examples 49997\n",
    "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.23583640158176422 Num fake examples 47878 Num true examples 50682\n",
    "  Batch 3,120  of  3,355.    Elapsed: 0:18:41. Training loss. 0.30059054493904114 Num fake examples 48508 Num true examples 51332\n",
    "  Batch 3,160  of  3,355.    Elapsed: 0:18:55. Training loss. 0.0451841726899147 Num fake examples 49124 Num true examples 51996\n",
    "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.3374781608581543 Num fake examples 49758 Num true examples 52642\n",
    "  Batch 3,240  of  3,355.    Elapsed: 0:19:24. Training loss. 0.04189993813633919 Num fake examples 50379 Num true examples 53301\n",
    "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.1328296661376953 Num fake examples 51008 Num true examples 53952\n",
    "  Batch 3,320  of  3,355.    Elapsed: 0:19:53. Training loss. 0.13455358147621155 Num fake examples 51616 Num true examples 54624\n",
    "\n",
    "  Average training loss: 0.18\n",
    "  Training epcoh took: 0:20:06\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.96\n",
    "  Validation Loss: 0.18\n",
    "  Validation took: 0:02:01\n",
    "\n",
    "======== Epoch 2 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.03935094550251961 Num fake examples 625 Num true examples 655\n",
    "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.12716390192508698 Num fake examples 1260 Num true examples 1300\n",
    "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.2671760618686676 Num fake examples 1868 Num true examples 1972\n",
    "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.5350862741470337 Num fake examples 2492 Num true examples 2628\n",
    "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.042097486555576324 Num fake examples 3087 Num true examples 3313\n",
    "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1543845236301422 Num fake examples 3726 Num true examples 3954\n",
    "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.14083221554756165 Num fake examples 4329 Num true examples 4631\n",
    "  Batch   320  of  3,355.    Elapsed: 0:01:55. Training loss. 0.03466573730111122 Num fake examples 4962 Num true examples 5278\n",
    "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.33024510741233826 Num fake examples 5571 Num true examples 5949\n",
    "  Batch   400  of  3,355.    Elapsed: 0:02:23. Training loss. 0.32197344303131104 Num fake examples 6164 Num true examples 6636\n",
    "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.11075636744499207 Num fake examples 6811 Num true examples 7269\n",
    "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.13908791542053223 Num fake examples 7466 Num true examples 7894\n",
    "  Batch   520  of  3,355.    Elapsed: 0:03:07. Training loss. 0.4561178982257843 Num fake examples 8066 Num true examples 8574\n",
    "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.1121193915605545 Num fake examples 8687 Num true examples 9233\n",
    "  Batch   600  of  3,355.    Elapsed: 0:03:35. Training loss. 0.25839942693710327 Num fake examples 9303 Num true examples 9897\n",
    "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.04425102472305298 Num fake examples 9937 Num true examples 10543\n",
    "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.3780013620853424 Num fake examples 10577 Num true examples 11183\n",
    "  Batch   720  of  3,355.    Elapsed: 0:04:19. Training loss. 0.133817657828331 Num fake examples 11185 Num true examples 11855\n",
    "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.041212279349565506 Num fake examples 11836 Num true examples 12484\n",
    "  Batch   800  of  3,355.    Elapsed: 0:04:47. Training loss. 0.24937984347343445 Num fake examples 12437 Num true examples 13163\n",
    "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.04223354160785675 Num fake examples 13058 Num true examples 13822\n",
    "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.03940189629793167 Num fake examples 13663 Num true examples 14497\n",
    "  Batch   920  of  3,355.    Elapsed: 0:05:30. Training loss. 0.12211575359106064 Num fake examples 14299 Num true examples 15141\n",
    "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.2688104212284088 Num fake examples 14920 Num true examples 15800\n",
    "  Batch 1,000  of  3,355.    Elapsed: 0:05:59. Training loss. 0.03407096117734909 Num fake examples 15545 Num true examples 16455\n",
    "  Batch 1,040  of  3,355.    Elapsed: 0:06:14. Training loss. 0.3238053321838379 Num fake examples 16171 Num true examples 17109\n",
    "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.25952139496803284 Num fake examples 16818 Num true examples 17742\n",
    "  Batch 1,120  of  3,355.    Elapsed: 0:06:42. Training loss. 0.11196809262037277 Num fake examples 17436 Num true examples 18404\n",
    "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.23539024591445923 Num fake examples 18036 Num true examples 19084\n",
    "  Batch 1,200  of  3,355.    Elapsed: 0:07:11. Training loss. 0.03345056250691414 Num fake examples 18676 Num true examples 19724\n",
    "  Batch 1,240  of  3,355.    Elapsed: 0:07:26. Training loss. 0.21026352047920227 Num fake examples 19284 Num true examples 20396\n",
    "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.03577909246087074 Num fake examples 19907 Num true examples 21053\n",
    "  Batch 1,320  of  3,355.    Elapsed: 0:07:54. Training loss. 0.11131535470485687 Num fake examples 20548 Num true examples 21692\n",
    "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.39729541540145874 Num fake examples 21170 Num true examples 22350\n",
    "  Batch 1,400  of  3,355.    Elapsed: 0:08:23. Training loss. 0.10656000673770905 Num fake examples 21795 Num true examples 23005\n",
    "  Batch 1,440  of  3,355.    Elapsed: 0:08:37. Training loss. 0.15148267149925232 Num fake examples 22403 Num true examples 23677\n",
    "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.04705808684229851 Num fake examples 23084 Num true examples 24276\n",
    "  Batch 1,520  of  3,355.    Elapsed: 0:09:06. Training loss. 0.14977127313613892 Num fake examples 23704 Num true examples 24936\n",
    "  Batch 1,560  of  3,355.    Elapsed: 0:09:20. Training loss. 0.03257656842470169 Num fake examples 24361 Num true examples 25559\n",
    "  Batch 1,600  of  3,355.    Elapsed: 0:09:35. Training loss. 0.13818959891796112 Num fake examples 25013 Num true examples 26187\n",
    "  Batch 1,640  of  3,355.    Elapsed: 0:09:49. Training loss. 0.02896132692694664 Num fake examples 25619 Num true examples 26861\n",
    "  Batch 1,680  of  3,355.    Elapsed: 0:10:04. Training loss. 0.2429731786251068 Num fake examples 26224 Num true examples 27536\n",
    "  Batch 1,720  of  3,355.    Elapsed: 0:10:18. Training loss. 0.19558961689472198 Num fake examples 26849 Num true examples 28191\n",
    "  Batch 1,760  of  3,355.    Elapsed: 0:10:32. Training loss. 0.1265896111726761 Num fake examples 27488 Num true examples 28832\n",
    "  Batch 1,800  of  3,355.    Elapsed: 0:10:47. Training loss. 0.14584015309810638 Num fake examples 28104 Num true examples 29496\n",
    "  Batch 1,840  of  3,355.    Elapsed: 0:11:01. Training loss. 0.11036056280136108 Num fake examples 28715 Num true examples 30165\n",
    "  Batch 1,880  of  3,355.    Elapsed: 0:11:15. Training loss. 0.2579878568649292 Num fake examples 29374 Num true examples 30786\n",
    "  Batch 1,920  of  3,355.    Elapsed: 0:11:30. Training loss. 0.1441950649023056 Num fake examples 29989 Num true examples 31451\n",
    "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.04405297338962555 Num fake examples 30605 Num true examples 32115\n",
    "  Batch 2,000  of  3,355.    Elapsed: 0:11:59. Training loss. 0.1581941694021225 Num fake examples 31247 Num true examples 32753\n",
    "  Batch 2,040  of  3,355.    Elapsed: 0:12:13. Training loss. 0.31049904227256775 Num fake examples 31851 Num true examples 33429\n",
    "  Batch 2,080  of  3,355.    Elapsed: 0:12:27. Training loss. 0.2443661093711853 Num fake examples 32466 Num true examples 34094\n",
    "  Batch 2,120  of  3,355.    Elapsed: 0:12:42. Training loss. 0.14016607403755188 Num fake examples 33066 Num true examples 34774\n",
    "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.04870529845356941 Num fake examples 33705 Num true examples 35415\n",
    "  Batch 2,200  of  3,355.    Elapsed: 0:13:10. Training loss. 0.11456242203712463 Num fake examples 34303 Num true examples 36097\n",
    "  Batch 2,240  of  3,355.    Elapsed: 0:13:25. Training loss. 0.27177000045776367 Num fake examples 34916 Num true examples 36764\n",
    "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.03737764060497284 Num fake examples 35527 Num true examples 37433\n",
    "  Batch 2,320  of  3,355.    Elapsed: 0:13:54. Training loss. 0.13009679317474365 Num fake examples 36181 Num true examples 38059\n",
    "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.05307362228631973 Num fake examples 36791 Num true examples 38729\n",
    "  Batch 2,400  of  3,355.    Elapsed: 0:14:22. Training loss. 0.26379427313804626 Num fake examples 37420 Num true examples 39380\n",
    "  Batch 2,440  of  3,355.    Elapsed: 0:14:37. Training loss. 0.2513780891895294 Num fake examples 38038 Num true examples 40042\n",
    "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.23142875730991364 Num fake examples 38639 Num true examples 40721\n",
    "  Batch 2,520  of  3,355.    Elapsed: 0:15:05. Training loss. 0.1414983719587326 Num fake examples 39221 Num true examples 41419\n",
    "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.23584167659282684 Num fake examples 39835 Num true examples 42085\n",
    "  Batch 2,600  of  3,355.    Elapsed: 0:15:34. Training loss. 0.2483856976032257 Num fake examples 40462 Num true examples 42738\n",
    "  Batch 2,640  of  3,355.    Elapsed: 0:15:49. Training loss. 0.24674564599990845 Num fake examples 41046 Num true examples 43434\n",
    "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.22001497447490692 Num fake examples 41684 Num true examples 44076\n",
    "  Batch 2,720  of  3,355.    Elapsed: 0:16:17. Training loss. 0.14037881791591644 Num fake examples 42318 Num true examples 44722\n",
    "  Batch 2,760  of  3,355.    Elapsed: 0:16:32. Training loss. 0.043773286044597626 Num fake examples 42939 Num true examples 45381\n",
    "  Batch 2,800  of  3,355.    Elapsed: 0:16:46. Training loss. 0.04199671000242233 Num fake examples 43580 Num true examples 46020\n",
    "  Batch 2,840  of  3,355.    Elapsed: 0:17:00. Training loss. 0.25485843420028687 Num fake examples 44199 Num true examples 46681\n",
    "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.3623812198638916 Num fake examples 44826 Num true examples 47334\n",
    "  Batch 2,920  of  3,355.    Elapsed: 0:17:29. Training loss. 0.20391181111335754 Num fake examples 45437 Num true examples 48003\n",
    "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.3336174488067627 Num fake examples 46064 Num true examples 48656\n",
    "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.03978920355439186 Num fake examples 46680 Num true examples 49320\n",
    "  Batch 3,040  of  3,355.    Elapsed: 0:18:12. Training loss. 0.231441929936409 Num fake examples 47307 Num true examples 49973\n",
    "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.2307634800672531 Num fake examples 47935 Num true examples 50625\n",
    "  Batch 3,120  of  3,355.    Elapsed: 0:18:41. Training loss. 0.25378817319869995 Num fake examples 48550 Num true examples 51290\n",
    "  Batch 3,160  of  3,355.    Elapsed: 0:18:56. Training loss. 0.3293209671974182 Num fake examples 49164 Num true examples 51956\n",
    "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.03976007550954819 Num fake examples 49797 Num true examples 52603\n",
    "  Batch 3,240  of  3,355.    Elapsed: 0:19:24. Training loss. 0.328784704208374 Num fake examples 50425 Num true examples 53255\n",
    "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.03953807055950165 Num fake examples 51025 Num true examples 53935\n",
    "  Batch 3,320  of  3,355.    Elapsed: 0:19:53. Training loss. 0.43083977699279785 Num fake examples 51634 Num true examples 54606\n",
    "\n",
    "  Average training loss: 0.17\n",
    "  Training epcoh took: 0:20:06\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.96\n",
    "  Validation Loss: 0.18\n",
    "  Validation took: 0:02:01\n",
    "\n",
    "======== Epoch 3 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.3338867425918579 Num fake examples 633 Num true examples 647\n",
    "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.24493588507175446 Num fake examples 1238 Num true examples 1322\n",
    "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.23917953670024872 Num fake examples 1862 Num true examples 1978\n",
    "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.04362295940518379 Num fake examples 2476 Num true examples 2644\n",
    "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.13870671391487122 Num fake examples 3082 Num true examples 3318\n",
    "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1341569870710373 Num fake examples 3708 Num true examples 3972\n",
    "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.13023115694522858 Num fake examples 4292 Num true examples 4668\n",
    "  Batch   320  of  3,355.    Elapsed: 0:01:54. Training loss. 0.2368895709514618 Num fake examples 4901 Num true examples 5339\n",
    "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.2234560251235962 Num fake examples 5509 Num true examples 6011\n",
    "  Batch   400  of  3,355.    Elapsed: 0:02:23. Training loss. 0.12225377559661865 Num fake examples 6113 Num true examples 6687\n",
    "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.22883129119873047 Num fake examples 6707 Num true examples 7373\n",
    "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.0419744998216629 Num fake examples 7328 Num true examples 8032\n",
    "  Batch   520  of  3,355.    Elapsed: 0:03:06. Training loss. 0.13669711351394653 Num fake examples 7955 Num true examples 8685\n",
    "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.04214046150445938 Num fake examples 8557 Num true examples 9363\n",
    "  Batch   600  of  3,355.    Elapsed: 0:03:35. Training loss. 0.04172424599528313 Num fake examples 9160 Num true examples 10040\n",
    "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.2394254207611084 Num fake examples 9781 Num true examples 10699\n",
    "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.13004055619239807 Num fake examples 10439 Num true examples 11321\n",
    "  Batch   720  of  3,355.    Elapsed: 0:04:18. Training loss. 0.03967522829771042 Num fake examples 11048 Num true examples 11992\n",
    "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.334077924489975 Num fake examples 11655 Num true examples 12665\n",
    "  Batch   800  of  3,355.    Elapsed: 0:04:47. Training loss. 0.214199960231781 Num fake examples 12267 Num true examples 13333\n",
    "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.21729257702827454 Num fake examples 12916 Num true examples 13964\n",
    "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.1504918783903122 Num fake examples 13539 Num true examples 14621\n",
    "  Batch   920  of  3,355.    Elapsed: 0:05:30. Training loss. 0.13153523206710815 Num fake examples 14158 Num true examples 15282\n",
    "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.04383350908756256 Num fake examples 14743 Num true examples 15977\n",
    "  Batch 1,000  of  3,355.    Elapsed: 0:05:59. Training loss. 0.03426191210746765 Num fake examples 15365 Num true examples 16635\n",
    "  Batch 1,040  of  3,355.    Elapsed: 0:06:13. Training loss. 0.03896559774875641 Num fake examples 15988 Num true examples 17292\n",
    "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.13873271644115448 Num fake examples 16614 Num true examples 17946\n",
    "  Batch 1,120  of  3,355.    Elapsed: 0:06:42. Training loss. 0.1565924882888794 Num fake examples 17269 Num true examples 18571\n",
    "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.11483988910913467 Num fake examples 17901 Num true examples 19219\n",
    "  Batch 1,200  of  3,355.    Elapsed: 0:07:11. Training loss. 0.0364774614572525 Num fake examples 18506 Num true examples 19894\n",
    "  Batch 1,240  of  3,355.    Elapsed: 0:07:25. Training loss. 0.036551475524902344 Num fake examples 19158 Num true examples 20522\n",
    "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.03574036806821823 Num fake examples 19761 Num true examples 21199\n",
    "  Batch 1,320  of  3,355.    Elapsed: 0:07:54. Training loss. 0.14958828687667847 Num fake examples 20374 Num true examples 21866\n",
    "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.12117072194814682 Num fake examples 20980 Num true examples 22540\n",
    "  Batch 1,400  of  3,355.    Elapsed: 0:08:23. Training loss. 0.037765443325042725 Num fake examples 21625 Num true examples 23175\n",
    "  Batch 1,440  of  3,355.    Elapsed: 0:08:37. Training loss. 0.0368092879652977 Num fake examples 22254 Num true examples 23826\n",
    "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.2496054470539093 Num fake examples 22902 Num true examples 24458\n",
    "  Batch 1,520  of  3,355.    Elapsed: 0:09:06. Training loss. 0.2884432375431061 Num fake examples 23515 Num true examples 25125\n",
    "  Batch 1,560  of  3,355.    Elapsed: 0:09:20. Training loss. 0.3019912838935852 Num fake examples 24144 Num true examples 25776\n",
    "  Batch 1,600  of  3,355.    Elapsed: 0:09:35. Training loss. 0.23881559073925018 Num fake examples 24764 Num true examples 26436\n",
    "  Batch 1,640  of  3,355.    Elapsed: 0:09:49. Training loss. 0.13872145116329193 Num fake examples 25405 Num true examples 27075\n",
    "  Batch 1,680  of  3,355.    Elapsed: 0:10:04. Training loss. 0.22398824989795685 Num fake examples 26023 Num true examples 27737\n",
    "  Batch 1,720  of  3,355.    Elapsed: 0:10:18. Training loss. 0.037736695259809494 Num fake examples 26656 Num true examples 28384\n",
    "  Batch 1,760  of  3,355.    Elapsed: 0:10:32. Training loss. 0.1363040655851364 Num fake examples 27294 Num true examples 29026\n",
    "  Batch 1,800  of  3,355.    Elapsed: 0:10:47. Training loss. 0.25522521138191223 Num fake examples 27911 Num true examples 29689\n",
    "  Batch 1,840  of  3,355.    Elapsed: 0:11:01. Training loss. 0.13717131316661835 Num fake examples 28525 Num true examples 30355\n",
    "  Batch 1,880  of  3,355.    Elapsed: 0:11:16. Training loss. 0.1466633826494217 Num fake examples 29164 Num true examples 30996\n",
    "  Batch 1,920  of  3,355.    Elapsed: 0:11:30. Training loss. 0.04241253063082695 Num fake examples 29775 Num true examples 31665\n",
    "  Batch 1,960  of  3,355.    Elapsed: 0:11:44. Training loss. 0.1977137327194214 Num fake examples 30384 Num true examples 32336\n",
    "  Batch 2,000  of  3,355.    Elapsed: 0:11:59. Training loss. 0.2531745433807373 Num fake examples 30992 Num true examples 33008\n",
    "  Batch 2,040  of  3,355.    Elapsed: 0:12:13. Training loss. 0.04097733274102211 Num fake examples 31652 Num true examples 33628\n",
    "  Batch 2,080  of  3,355.    Elapsed: 0:12:28. Training loss. 0.040315769612789154 Num fake examples 32286 Num true examples 34274\n",
    "  Batch 2,120  of  3,355.    Elapsed: 0:12:42. Training loss. 0.26836201548576355 Num fake examples 32926 Num true examples 34914\n",
    "  Batch 2,160  of  3,355.    Elapsed: 0:12:56. Training loss. 0.1515481173992157 Num fake examples 33532 Num true examples 35588\n",
    "  Batch 2,200  of  3,355.    Elapsed: 0:13:11. Training loss. 0.04283876344561577 Num fake examples 34138 Num true examples 36262\n",
    "  Batch 2,240  of  3,355.    Elapsed: 0:13:25. Training loss. 0.12743979692459106 Num fake examples 34753 Num true examples 36927\n",
    "  Batch 2,280  of  3,355.    Elapsed: 0:13:39. Training loss. 0.03976508229970932 Num fake examples 35372 Num true examples 37588\n",
    "  Batch 2,320  of  3,355.    Elapsed: 0:13:54. Training loss. 0.15163537859916687 Num fake examples 36002 Num true examples 38238\n",
    "  Batch 2,360  of  3,355.    Elapsed: 0:14:08. Training loss. 0.32829177379608154 Num fake examples 36641 Num true examples 38879\n",
    "  Batch 2,400  of  3,355.    Elapsed: 0:14:23. Training loss. 0.31207674741744995 Num fake examples 37299 Num true examples 39501\n",
    "  Batch 2,440  of  3,355.    Elapsed: 0:14:37. Training loss. 0.11240227520465851 Num fake examples 37915 Num true examples 40165\n",
    "  Batch 2,480  of  3,355.    Elapsed: 0:14:51. Training loss. 0.11144663393497467 Num fake examples 38575 Num true examples 40785\n",
    "  Batch 2,520  of  3,355.    Elapsed: 0:15:06. Training loss. 0.23262450098991394 Num fake examples 39212 Num true examples 41428\n",
    "  Batch 2,560  of  3,355.    Elapsed: 0:15:20. Training loss. 0.24706904590129852 Num fake examples 39823 Num true examples 42097\n",
    "  Batch 2,600  of  3,355.    Elapsed: 0:15:35. Training loss. 0.11494562774896622 Num fake examples 40406 Num true examples 42794\n",
    "  Batch 2,640  of  3,355.    Elapsed: 0:15:49. Training loss. 0.3657858967781067 Num fake examples 41031 Num true examples 43449\n",
    "  Batch 2,680  of  3,355.    Elapsed: 0:16:03. Training loss. 0.16093550622463226 Num fake examples 41654 Num true examples 44106\n",
    "  Batch 2,720  of  3,355.    Elapsed: 0:16:18. Training loss. 0.04275315999984741 Num fake examples 42280 Num true examples 44760\n",
    "  Batch 2,760  of  3,355.    Elapsed: 0:16:32. Training loss. 0.1517629623413086 Num fake examples 42882 Num true examples 45438\n",
    "  Batch 2,800  of  3,355.    Elapsed: 0:16:47. Training loss. 0.15819057822227478 Num fake examples 43529 Num true examples 46071\n",
    "  Batch 2,840  of  3,355.    Elapsed: 0:17:01. Training loss. 0.22100567817687988 Num fake examples 44139 Num true examples 46741\n",
    "  Batch 2,880  of  3,355.    Elapsed: 0:17:15. Training loss. 0.23929274082183838 Num fake examples 44770 Num true examples 47390\n",
    "  Batch 2,920  of  3,355.    Elapsed: 0:17:30. Training loss. 0.4428822994232178 Num fake examples 45425 Num true examples 48015\n",
    "  Batch 2,960  of  3,355.    Elapsed: 0:17:44. Training loss. 0.3581375479698181 Num fake examples 46026 Num true examples 48694\n",
    "  Batch 3,000  of  3,355.    Elapsed: 0:17:58. Training loss. 0.22258593142032623 Num fake examples 46649 Num true examples 49351\n",
    "  Batch 3,040  of  3,355.    Elapsed: 0:18:13. Training loss. 0.24387116730213165 Num fake examples 47274 Num true examples 50006\n",
    "  Batch 3,080  of  3,355.    Elapsed: 0:18:27. Training loss. 0.2356257438659668 Num fake examples 47911 Num true examples 50649\n",
    "  Batch 3,120  of  3,355.    Elapsed: 0:18:42. Training loss. 0.4521031677722931 Num fake examples 48541 Num true examples 51299\n",
    "  Batch 3,160  of  3,355.    Elapsed: 0:18:56. Training loss. 0.13446354866027832 Num fake examples 49144 Num true examples 51976\n",
    "  Batch 3,200  of  3,355.    Elapsed: 0:19:10. Training loss. 0.03812853619456291 Num fake examples 49787 Num true examples 52613\n",
    "  Batch 3,240  of  3,355.    Elapsed: 0:19:25. Training loss. 0.043760865926742554 Num fake examples 50413 Num true examples 53267\n",
    "  Batch 3,280  of  3,355.    Elapsed: 0:19:39. Training loss. 0.1384311467409134 Num fake examples 51017 Num true examples 53943\n",
    "  Batch 3,320  of  3,355.    Elapsed: 0:19:54. Training loss. 0.13893994688987732 Num fake examples 51629 Num true examples 54611\n",
    "\n",
    "  Average training loss: 0.17\n",
    "  Training epcoh took: 0:20:06\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.96\n",
    "  Validation Loss: 0.18\n",
    "  Validation took: 0:02:01\n",
    "\n",
    "======== Epoch 4 / 4 ========\n",
    "Training...\n",
    "  Batch    40  of  3,355.    Elapsed: 0:00:14. Training loss. 0.2409074604511261 Num fake examples 625 Num true examples 655\n",
    "  Batch    80  of  3,355.    Elapsed: 0:00:28. Training loss. 0.03874039649963379 Num fake examples 1240 Num true examples 1320\n",
    "  Batch   120  of  3,355.    Elapsed: 0:00:43. Training loss. 0.23137454688549042 Num fake examples 1864 Num true examples 1976\n",
    "  Batch   160  of  3,355.    Elapsed: 0:00:57. Training loss. 0.04342831298708916 Num fake examples 2494 Num true examples 2626\n",
    "  Batch   200  of  3,355.    Elapsed: 0:01:11. Training loss. 0.043720610439777374 Num fake examples 3124 Num true examples 3276\n",
    "  Batch   240  of  3,355.    Elapsed: 0:01:26. Training loss. 0.1579982191324234 Num fake examples 3738 Num true examples 3942\n",
    "  Batch   280  of  3,355.    Elapsed: 0:01:40. Training loss. 0.037048667669296265 Num fake examples 4352 Num true examples 4608\n",
    "  Batch   320  of  3,355.    Elapsed: 0:01:55. Training loss. 0.03720291703939438 Num fake examples 4987 Num true examples 5253\n",
    "  Batch   360  of  3,355.    Elapsed: 0:02:09. Training loss. 0.10871105641126633 Num fake examples 5630 Num true examples 5890\n",
    "  Batch   400  of  3,355.    Elapsed: 0:02:24. Training loss. 0.037721361964941025 Num fake examples 6245 Num true examples 6555\n",
    "  Batch   440  of  3,355.    Elapsed: 0:02:38. Training loss. 0.3096107244491577 Num fake examples 6869 Num true examples 7211\n",
    "  Batch   480  of  3,355.    Elapsed: 0:02:52. Training loss. 0.14429622888565063 Num fake examples 7470 Num true examples 7890\n",
    "  Batch   520  of  3,355.    Elapsed: 0:03:07. Training loss. 0.1458272784948349 Num fake examples 8112 Num true examples 8528\n",
    "  Batch   560  of  3,355.    Elapsed: 0:03:21. Training loss. 0.10995736718177795 Num fake examples 8746 Num true examples 9174\n",
    "  Batch   600  of  3,355.    Elapsed: 0:03:36. Training loss. 0.03944821655750275 Num fake examples 9396 Num true examples 9804\n",
    "  Batch   640  of  3,355.    Elapsed: 0:03:50. Training loss. 0.3315431773662567 Num fake examples 10017 Num true examples 10463\n",
    "  Batch   680  of  3,355.    Elapsed: 0:04:04. Training loss. 0.23462273180484772 Num fake examples 10641 Num true examples 11119\n",
    "  Batch   720  of  3,355.    Elapsed: 0:04:19. Training loss. 0.16211122274398804 Num fake examples 11242 Num true examples 11798\n",
    "  Batch   760  of  3,355.    Elapsed: 0:04:33. Training loss. 0.03663318604230881 Num fake examples 11902 Num true examples 12418\n",
    "  Batch   800  of  3,355.    Elapsed: 0:04:48. Training loss. 0.23556575179100037 Num fake examples 12510 Num true examples 13090\n",
    "  Batch   840  of  3,355.    Elapsed: 0:05:02. Training loss. 0.2568124830722809 Num fake examples 13147 Num true examples 13733\n",
    "  Batch   880  of  3,355.    Elapsed: 0:05:16. Training loss. 0.41200733184814453 Num fake examples 13765 Num true examples 14395\n",
    "  Batch   920  of  3,355.    Elapsed: 0:05:31. Training loss. 0.23185721039772034 Num fake examples 14424 Num true examples 15016\n",
    "  Batch   960  of  3,355.    Elapsed: 0:05:45. Training loss. 0.041035495698451996 Num fake examples 15065 Num true examples 15655\n",
    "  Batch 1,000  of  3,355.    Elapsed: 0:06:00. Training loss. 0.23801107704639435 Num fake examples 15668 Num true examples 16332\n",
    "  Batch 1,040  of  3,355.    Elapsed: 0:06:14. Training loss. 0.15904563665390015 Num fake examples 16286 Num true examples 16994\n",
    "  Batch 1,080  of  3,355.    Elapsed: 0:06:28. Training loss. 0.2100268006324768 Num fake examples 16902 Num true examples 17658\n",
    "  Batch 1,120  of  3,355.    Elapsed: 0:06:43. Training loss. 0.24760288000106812 Num fake examples 17516 Num true examples 18324\n",
    "  Batch 1,160  of  3,355.    Elapsed: 0:06:57. Training loss. 0.04195927083492279 Num fake examples 18107 Num true examples 19013\n",
    "  Batch 1,200  of  3,355.    Elapsed: 0:07:12. Training loss. 0.2511937916278839 Num fake examples 18718 Num true examples 19682\n",
    "  Batch 1,240  of  3,355.    Elapsed: 0:07:26. Training loss. 0.03692358732223511 Num fake examples 19328 Num true examples 20352\n",
    "  Batch 1,280  of  3,355.    Elapsed: 0:07:40. Training loss. 0.04175263270735741 Num fake examples 19982 Num true examples 20978\n",
    "  Batch 1,320  of  3,355.    Elapsed: 0:07:55. Training loss. 0.1322416216135025 Num fake examples 20580 Num true examples 21660\n",
    "  Batch 1,360  of  3,355.    Elapsed: 0:08:09. Training loss. 0.1274494081735611 Num fake examples 21193 Num true examples 22327\n",
    "  Batch 1,400  of  3,355.    Elapsed: 0:08:24. Training loss. 0.1172788217663765 Num fake examples 21816 Num true examples 22984\n",
    "  Batch 1,440  of  3,355.    Elapsed: 0:08:38. Training loss. 0.24210676550865173 Num fake examples 22417 Num true examples 23663\n",
    "  Batch 1,480  of  3,355.    Elapsed: 0:08:52. Training loss. 0.03667015582323074 Num fake examples 23047 Num true examples 24313\n",
    "  Batch 1,520  of  3,355.    Elapsed: 0:09:07. Training loss. 0.31395086646080017 Num fake examples 23650 Num true examples 24990\n",
    "  Batch 1,560  of  3,355.    Elapsed: 0:09:21. Training loss. 0.16145384311676025 Num fake examples 24278 Num true examples 25642\n",
    "  Batch 1,600  of  3,355.    Elapsed: 0:09:36. Training loss. 0.15182346105575562 Num fake examples 24877 Num true examples 26323\n",
    "  Batch 1,640  of  3,355.    Elapsed: 0:09:50. Training loss. 0.21085284650325775 Num fake examples 25514 Num true examples 26966\n",
    "  Batch 1,680  of  3,355.    Elapsed: 0:10:05. Training loss. 0.1323704868555069 Num fake examples 26101 Num true examples 27659\n",
    "  Batch 1,720  of  3,355.    Elapsed: 0:10:19. Training loss. 0.1545555293560028 Num fake examples 26734 Num true examples 28306\n",
    "  Batch 1,760  of  3,355.    Elapsed: 0:10:33. Training loss. 0.13633529841899872 Num fake examples 27362 Num true examples 28958\n",
    "  Batch 1,800  of  3,355.    Elapsed: 0:10:48. Training loss. 0.136884868144989 Num fake examples 28005 Num true examples 29595\n",
    "  Batch 1,840  of  3,355.    Elapsed: 0:11:02. Training loss. 0.12205293774604797 Num fake examples 28596 Num true examples 30284\n",
    "  Batch 1,880  of  3,355.    Elapsed: 0:11:17. Training loss. 0.23902100324630737 Num fake examples 29193 Num true examples 30967\n",
    "  Batch 1,920  of  3,355.    Elapsed: 0:11:31. Training loss. 0.15080493688583374 Num fake examples 29822 Num true examples 31618\n",
    "  Batch 1,960  of  3,355.    Elapsed: 0:11:45. Training loss. 0.047677673399448395 Num fake examples 30434 Num true examples 32286\n",
    "  Batch 2,000  of  3,355.    Elapsed: 0:12:00. Training loss. 0.03955874219536781 Num fake examples 31074 Num true examples 32926\n",
    "  Batch 2,040  of  3,355.    Elapsed: 0:12:14. Training loss. 0.23810961842536926 Num fake examples 31682 Num true examples 33598\n",
    "  Batch 2,080  of  3,355.    Elapsed: 0:12:29. Training loss. 0.24548694491386414 Num fake examples 32316 Num true examples 34244\n",
    "  Batch 2,120  of  3,355.    Elapsed: 0:12:43. Training loss. 0.3976697623729706 Num fake examples 32946 Num true examples 34894\n",
    "  Batch 2,160  of  3,355.    Elapsed: 0:12:57. Training loss. 0.043257374316453934 Num fake examples 33563 Num true examples 35557\n",
    "  Batch 2,200  of  3,355.    Elapsed: 0:13:12. Training loss. 0.03786439076066017 Num fake examples 34201 Num true examples 36199\n",
    "  Batch 2,240  of  3,355.    Elapsed: 0:13:26. Training loss. 0.03973526880145073 Num fake examples 34815 Num true examples 36865\n",
    "  Batch 2,280  of  3,355.    Elapsed: 0:13:41. Training loss. 0.32164761424064636 Num fake examples 35448 Num true examples 37512\n",
    "  Batch 2,320  of  3,355.    Elapsed: 0:13:55. Training loss. 0.1384255588054657 Num fake examples 36051 Num true examples 38189\n",
    "  Batch 2,360  of  3,355.    Elapsed: 0:14:10. Training loss. 0.14196622371673584 Num fake examples 36663 Num true examples 38857\n",
    "  Batch 2,400  of  3,355.    Elapsed: 0:14:24. Training loss. 0.24955777823925018 Num fake examples 37285 Num true examples 39515\n",
    "  Batch 2,440  of  3,355.    Elapsed: 0:14:38. Training loss. 0.14557817578315735 Num fake examples 37908 Num true examples 40172\n",
    "  Batch 2,480  of  3,355.    Elapsed: 0:14:53. Training loss. 0.23301628232002258 Num fake examples 38539 Num true examples 40821\n",
    "  Batch 2,520  of  3,355.    Elapsed: 0:15:07. Training loss. 0.04076351225376129 Num fake examples 39178 Num true examples 41462\n",
    "  Batch 2,560  of  3,355.    Elapsed: 0:15:22. Training loss. 0.04160863161087036 Num fake examples 39790 Num true examples 42130\n",
    "  Batch 2,600  of  3,355.    Elapsed: 0:15:36. Training loss. 0.038130082190036774 Num fake examples 40434 Num true examples 42766\n",
    "  Batch 2,640  of  3,355.    Elapsed: 0:15:50. Training loss. 0.15199041366577148 Num fake examples 41086 Num true examples 43394\n",
    "  Batch 2,680  of  3,355.    Elapsed: 0:16:05. Training loss. 0.04173336550593376 Num fake examples 41713 Num true examples 44047\n",
    "  Batch 2,720  of  3,355.    Elapsed: 0:16:19. Training loss. 0.04318900778889656 Num fake examples 42331 Num true examples 44709\n",
    "  Batch 2,760  of  3,355.    Elapsed: 0:16:34. Training loss. 0.13546667993068695 Num fake examples 42956 Num true examples 45364\n",
    "  Batch 2,800  of  3,355.    Elapsed: 0:16:48. Training loss. 0.04681965708732605 Num fake examples 43567 Num true examples 46033\n",
    "  Batch 2,840  of  3,355.    Elapsed: 0:17:03. Training loss. 0.248200923204422 Num fake examples 44209 Num true examples 46671\n",
    "  Batch 2,880  of  3,355.    Elapsed: 0:17:17. Training loss. 0.31713253259658813 Num fake examples 44816 Num true examples 47344\n",
    "  Batch 2,920  of  3,355.    Elapsed: 0:17:31. Training loss. 0.20648923516273499 Num fake examples 45438 Num true examples 48002\n",
    "  Batch 2,960  of  3,355.    Elapsed: 0:17:46. Training loss. 0.14694374799728394 Num fake examples 46055 Num true examples 48665\n",
    "  Batch 3,000  of  3,355.    Elapsed: 0:18:00. Training loss. 0.12243640422821045 Num fake examples 46677 Num true examples 49323\n",
    "  Batch 3,040  of  3,355.    Elapsed: 0:18:15. Training loss. 0.4050387144088745 Num fake examples 47276 Num true examples 50004\n",
    "  Batch 3,080  of  3,355.    Elapsed: 0:18:29. Training loss. 0.04104367271065712 Num fake examples 47902 Num true examples 50658\n",
    "  Batch 3,120  of  3,355.    Elapsed: 0:18:43. Training loss. 0.043901000171899796 Num fake examples 48506 Num true examples 51334\n",
    "  Batch 3,160  of  3,355.    Elapsed: 0:18:58. Training loss. 0.2628294825553894 Num fake examples 49147 Num true examples 51973\n",
    "  Batch 3,200  of  3,355.    Elapsed: 0:19:12. Training loss. 0.23191672563552856 Num fake examples 49770 Num true examples 52630\n",
    "  Batch 3,240  of  3,355.    Elapsed: 0:19:27. Training loss. 0.22508689761161804 Num fake examples 50398 Num true examples 53282\n",
    "  Batch 3,280  of  3,355.    Elapsed: 0:19:41. Training loss. 0.35537004470825195 Num fake examples 51025 Num true examples 53935\n",
    "  Batch 3,320  of  3,355.    Elapsed: 0:19:55. Training loss. 0.04083313047885895 Num fake examples 51636 Num true examples 54604\n",
    "\n",
    "  Average training loss: 0.17\n",
    "  Training epcoh took: 0:20:08\n",
    "\n",
    "Running Validation...\n",
    "  Accuracy: 0.96\n",
    "  Validation Loss: 0.18\n",
    "  Validation took: 0:02:01\n",
    "\n",
    "Training complete!\n",
    "Total training took 1:28:33 (h:mm:ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad475b4f-99d7-49e8-a6ca-512e908f3154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
