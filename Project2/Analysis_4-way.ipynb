{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9dd5351-f5ae-4a12-bac6-aefa206a3ec0",
   "metadata": {},
   "source": [
    "# Results from Initial run were better before adjusting the learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f4220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Displays the total number of rows (sentences) in the dataset.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data. \n",
    "# Shuffles the rows in the dataset randomly to ensure randomness in training and testing splits.\n",
    "# frac=1 ensures all rows are included in the shuffled DataFrame.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the statement and tweet columns into a single string for each row.\n",
    "# Example output: A formatted sentence string that concatenates the statement and a related tweet.\n",
    "#sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "\n",
    "df['statement'] = df['statement'].fillna(\"No statement provided\").str.strip()\n",
    "df['tweet'] = df['tweet'].fillna(\"No tweet provided\").str.strip()\n",
    "#sentences = 'Statement: ' + df['statement'] + ' || Tweet: ' + df['tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caac61c-165e-4325-83cd-08de139abaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the binary target labels (True or False) for classification.\n",
    "# labels will be used for training the model.\n",
    "# labels = df[\"BinaryNumTarget\"].values\n",
    "assert set(df[\"BinaryNumTarget\"].unique()).issubset({0, 1}), \"Invalid labels detected!\"\n",
    "labels = df[\"BinaryNumTarget\"].fillna(0).astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64918c31-13ca-442b-828c-c975a24b3b2e",
   "metadata": {},
   "source": [
    "### These functions determine the label (truthfulness) for each row based on the combination of the target value and the majority answer.\n",
    "#### Logic for 4-way function:\n",
    "Handles four categories of truthfulness: \"True\", \"False\", \"Mostly True\", \"Mostly False\".\n",
    "The label is flipped if target is False (e.g., \"Agree\" becomes \"False\").\n",
    "#### Logic for 2-way function:\n",
    "Handles binary truthfulness: \"True\" or \"False\".\n",
    "Simplifies the decision by considering only two categories of majority answers: \"Agree\" and \"Disagree\".\n",
    "#### Conditionals: \n",
    "The nested if-elif structure ensures that each row is assigned exactly one label based on its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    # Check if the 'target' column in the current row is True\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly False\"\n",
    "    else:  # If the 'target' column is not True (i.e., False)\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly True\"\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    # Check if the 'target' column in the current row is True\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:  # If the 'target' column is not True (i.e., False)\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd1a45-7c95-49df-b0fa-01026c42c601",
   "metadata": {},
   "source": [
    "### This step processes the data for classification tasks:\n",
    "4-way classification for nuanced truthfulness.\n",
    "2-way classification for simpler binary truthfulness.\n",
    "These labels can then be used as target variables for training machine learning or deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = df.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "df2['2-way-label'] = df.apply(lambda x: generate_truthfulness_2way(x), axis=1)\n",
    "df2['sentence'] = 'Statement: ' + df.loc[df2.index, 'statement'] + ' || Tweet: ' + df.loc[df2.index, 'tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: U.S. Rep. Matt Gaetz was the lone v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68538</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: \"The Obama administration has used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124532</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Statement: Says a video shared to his social m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95870</th>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Statement: Says a video captures sound from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63406</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: \"Did you know US population growth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: Small trials to test convalescent p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46160</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: \"White people control almost 90 per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132301</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>False</td>\n",
       "      <td>Statement: Since Trump labeled ANTIFA a terror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: The president's own FBI director sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66958</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "      <td>Statement: Says the U.S. Supreme Court found t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134198 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         4-way-label 2-way-label  \\\n",
       "4023            None        True   \n",
       "68538    Mostly True        True   \n",
       "124532         False       False   \n",
       "95870           None       False   \n",
       "63406           True        True   \n",
       "...              ...         ...   \n",
       "8271     Mostly True        True   \n",
       "46160           True        True   \n",
       "132301  Mostly False       False   \n",
       "4597     Mostly True        True   \n",
       "66958    Mostly True        True   \n",
       "\n",
       "                                                 sentence  \n",
       "4023    Statement: U.S. Rep. Matt Gaetz was the lone v...  \n",
       "68538   Statement: \"The Obama administration has used ...  \n",
       "124532  Statement: Says a video shared to his social m...  \n",
       "95870   Statement: Says a video captures sound from th...  \n",
       "63406   Statement: \"Did you know US population growth ...  \n",
       "...                                                   ...  \n",
       "8271    Statement: Small trials to test convalescent p...  \n",
       "46160   Statement: \"White people control almost 90 per...  \n",
       "132301  Statement: Since Trump labeled ANTIFA a terror...  \n",
       "4597    Statement: The president's own FBI director sa...  \n",
       "66958   Statement: Says the U.S. Supreme Court found t...  \n",
       "\n",
       "[134198 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_452306/1086030133.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['2-way-label'] = df2['2-way-label'].replace(label_mapping_2way)\n",
      "/tmp/ipykernel_452306/1086030133.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['4-way-label'] = df2['4-way-label'].replace(label_mapping_4way)\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with null values in labels\n",
    "df2 = df2[df2['2-way-label'].notnull()]\n",
    "df2 = df2[df2['4-way-label'].notnull()]\n",
    "\n",
    "# Replace string labels with numerical values\n",
    "label_mapping_2way = {'True': 0, 'False': 1}\n",
    "label_mapping_4way = {\n",
    "    \"True\": 0,\n",
    "    \"Mostly True\": 1,\n",
    "    \"Mostly False\": 2,\n",
    "    \"False\": 3\n",
    "}\n",
    "\n",
    "df2['2-way-label'] = df2['2-way-label'].replace(label_mapping_2way)\n",
    "df2['4-way-label'] = df2['4-way-label'].replace(label_mapping_4way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68538</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"The Obama administration has used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124532</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Says a video shared to his social m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"Did you know US population growth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54834</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: There were no guns whatsoever at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85801</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Aluminum is in the vaccine and will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: Small trials to test convalescent p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"White people control almost 90 per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132301</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Since Trump labeled ANTIFA a terror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: The president's own FBI director sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66958</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: Says the U.S. Supreme Court found t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label  2-way-label  \\\n",
       "68538             1            0   \n",
       "124532            3            1   \n",
       "63406             0            0   \n",
       "54834             1            0   \n",
       "85801             3            1   \n",
       "...             ...          ...   \n",
       "8271              1            0   \n",
       "46160             0            0   \n",
       "132301            2            1   \n",
       "4597              1            0   \n",
       "66958             1            0   \n",
       "\n",
       "                                                 sentence  \n",
       "68538   Statement: \"The Obama administration has used ...  \n",
       "124532  Statement: Says a video shared to his social m...  \n",
       "63406   Statement: \"Did you know US population growth ...  \n",
       "54834   Statement: There were no guns whatsoever at th...  \n",
       "85801   Statement: Aluminum is in the vaccine and will...  \n",
       "...                                                   ...  \n",
       "8271    Statement: Small trials to test convalescent p...  \n",
       "46160   Statement: \"White people control almost 90 per...  \n",
       "132301  Statement: Since Trump labeled ANTIFA a terror...  \n",
       "4597    Statement: The president's own FBI director sa...  \n",
       "66958   Statement: Says the U.S. Supreme Court found t...  \n",
       "\n",
       "[111593 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107bffd8-19dd-4d4d-8249-5544d33b2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 111,593\n",
      "Number of 4-way labels: 111,593\n",
      "Number of 2-way labels: 111,593\n"
     ]
    }
   ],
   "source": [
    "# Prepare variables for further processing\n",
    "sentences =  df2['sentence'].values\n",
    "labels_4way = df2['4-way-label'].values\n",
    "labels_2way = df2['2-way-label'].values\n",
    "\n",
    "# Display some debug information\n",
    "print(f\"Number of sentences: {len(sentences):,}\")\n",
    "print(f\"Number of 4-way labels: {len(labels_4way):,}\")\n",
    "print(f\"Number of 2-way labels: {len(labels_2way):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        4-way-label  2-way-label  \\\n",
      "68538             1            0   \n",
      "124532            3            1   \n",
      "63406             0            0   \n",
      "54834             1            0   \n",
      "85801             3            1   \n",
      "...             ...          ...   \n",
      "8271              1            0   \n",
      "46160             0            0   \n",
      "132301            2            1   \n",
      "4597              1            0   \n",
      "66958             1            0   \n",
      "\n",
      "                                                 sentence  \n",
      "68538   Statement: \"The Obama administration has used ...  \n",
      "124532  Statement: Says a video shared to his social m...  \n",
      "63406   Statement: \"Did you know US population growth ...  \n",
      "54834   Statement: There were no guns whatsoever at th...  \n",
      "85801   Statement: Aluminum is in the vaccine and will...  \n",
      "...                                                   ...  \n",
      "8271    Statement: Small trials to test convalescent p...  \n",
      "46160   Statement: \"White people control almost 90 per...  \n",
      "132301  Statement: Since Trump labeled ANTIFA a terror...  \n",
      "4597    Statement: The president's own FBI director sa...  \n",
      "66958   Statement: Says the U.S. Supreme Court found t...  \n",
      "\n",
      "[111593 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a histogram to visualize the distribution of the values in the 4-way-label column.\n",
    "df2['4-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes the numerical values (0 and 1) from the 4-way-label column in df2 and converts them into a NumPy array using the .values attribute.\n",
    "\n",
    "\n",
    "labels = df2['4-way-label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc333ff-fc92-4610-8655-a384c065ca06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68538</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"The Obama administration has used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124532</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Says a video shared to his social m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"Did you know US population growth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54834</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: There were no guns whatsoever at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85801</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Aluminum is in the vaccine and will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: Small trials to test convalescent p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: \"White people control almost 90 per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132301</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Statement: Since Trump labeled ANTIFA a terror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: The president's own FBI director sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66958</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Statement: Says the U.S. Supreme Court found t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label  2-way-label  \\\n",
       "68538             1            0   \n",
       "124532            3            1   \n",
       "63406             0            0   \n",
       "54834             1            0   \n",
       "85801             3            1   \n",
       "...             ...          ...   \n",
       "8271              1            0   \n",
       "46160             0            0   \n",
       "132301            2            1   \n",
       "4597              1            0   \n",
       "66958             1            0   \n",
       "\n",
       "                                                 sentence  \n",
       "68538   Statement: \"The Obama administration has used ...  \n",
       "124532  Statement: Says a video shared to his social m...  \n",
       "63406   Statement: \"Did you know US population growth ...  \n",
       "54834   Statement: There were no guns whatsoever at th...  \n",
       "85801   Statement: Aluminum is in the vaccine and will...  \n",
       "...                                                   ...  \n",
       "8271    Statement: Small trials to test convalescent p...  \n",
       "46160   Statement: \"White people control almost 90 per...  \n",
       "132301  Statement: Since Trump labeled ANTIFA a terror...  \n",
       "4597    Statement: The president's own FBI director sa...  \n",
       "66958   Statement: Says the U.S. Supreme Court found t...  \n",
       "\n",
       "[111593 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb700d8-a527-4fe6-aeed-ca76ce193e5f",
   "metadata": {},
   "source": [
    "### Loads a pre-trained BERT tokenizer from the Hugging Face model hub.\n",
    "'bert-base-uncased' specifies the version of the tokenizer:\n",
    "bert-base: A smaller, general-purpose BERT model with ~110 million parameters.\n",
    "uncased: The tokenizer will convert all text to lowercase and ignore case distinctions.\n",
    "\n",
    "#### The tokenizer converts input text into tokens (subword units) and maps them to integer IDs based on the BERT vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b229245-2b18-402c-a432-ae9bfb56106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Statement: \"The Obama administration has used the Espionage Act to go after whistleblowers who leaked to journalists ... more than all previous administrations combined.\" || Tweet: @Goldsto4Patrick @SethAbramson Oh yes, cause Trump was the one who signed the NDAA &amp; Corporate Espionage Acts or \n",
      "gave us the Patriot Act?\n",
      "\n",
      "No, that was Obama, Clinton, and Bush.\n",
      "\n",
      "You're the one who needs to wake up.\n",
      "Tokenized:  ['statement', ':', '\"', 'the', 'obama', 'administration', 'has', 'used', 'the', 'espionage', 'act', 'to', 'go', 'after', 'whistle', '##bl', '##ower', '##s', 'who', 'leaked', 'to', 'journalists', '.', '.', '.', 'more', 'than', 'all', 'previous', 'administrations', 'combined', '.', '\"', '|', '|', 't', '##wee', '##t', ':', '@', 'gold', '##sto', '##4', '##pa', '##trick', '@', 'seth', '##ab', '##ram', '##son', 'oh', 'yes', ',', 'cause', 'trump', 'was', 'the', 'one', 'who', 'signed', 'the', 'n', '##da', '##a', '&', 'amp', ';', 'corporate', 'espionage', 'acts', 'or', 'gave', 'us', 'the', 'patriot', 'act', '?', 'no', ',', 'that', 'was', 'obama', ',', 'clinton', ',', 'and', 'bush', '.', 'you', \"'\", 're', 'the', 'one', 'who', 'needs', 'to', 'wake', 'up', '.']\n",
      "Token IDs:  [4861, 1024, 1000, 1996, 8112, 3447, 2038, 2109, 1996, 21003, 2552, 2000, 2175, 2044, 13300, 16558, 25114, 2015, 2040, 15748, 2000, 8845, 1012, 1012, 1012, 2062, 2084, 2035, 3025, 27722, 4117, 1012, 1000, 1064, 1064, 1056, 28394, 2102, 1024, 1030, 2751, 16033, 2549, 4502, 22881, 1030, 6662, 7875, 6444, 3385, 2821, 2748, 1010, 3426, 8398, 2001, 1996, 2028, 2040, 2772, 1996, 1050, 2850, 2050, 1004, 23713, 1025, 5971, 21003, 4490, 2030, 2435, 2149, 1996, 16419, 2552, 1029, 2053, 1010, 2008, 2001, 8112, 1010, 7207, 1010, 1998, 5747, 1012, 2017, 1005, 2128, 1996, 2028, 2040, 3791, 2000, 5256, 2039, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05edcb-0519-4d9e-a939-0061c23d5861",
   "metadata": {},
   "source": [
    "#### Tokenization: Converts text into a form BERT can process.\n",
    "#### Numerical Mapping: Prepares the data for computation inside the model.\n",
    "#### Understanding: Lets you see exactly how the raw sentence transforms into something the model understands.\n",
    "    \n",
    "### Position in the Sequence:\n",
    "#### [CLS] is always at the start (index 0).\n",
    "##### [SEP] marks the end of a sentence or separates two sentences.\n",
    "### Processing:\n",
    "When BERT processes the input, it assigns special meanings to these tokens:\n",
    "#### The [CLS] token gathers the information from the entire sequence for classification tasks.\n",
    "#### The [SEP] token helps differentiate between sentence segments, improving context understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 111593/111593 [01:05<00:00, 1694.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c454f0d-ca3a-460e-8a26-851806617b87",
   "metadata": {},
   "source": [
    "### This step is foundational for using BERT because it:\n",
    "\n",
    "#### Prepares the data in a numerical, model-compatible format.\n",
    "#### Ensures consistent input length.\n",
    "#### Incorporates special tokens for proper processing by BERT.\n",
    "#### Provides attention masks to focus the model on meaningful parts of the data.\n",
    "Without this preprocessing step, the BERT model would not be able to understand or process your data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fbe87e0-a483-4ab3-8670-03b8241fc9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c83d070-225a-42bc-95f1-796879ea899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "text = \"Test sentence for PyTorch tensor\"\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=10,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',  # Ensure PyTorch tensors are requested\n",
    ")\n",
    "\n",
    "print(encoded_dict['input_ids'].device)  # Should print 'cpu' or 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 111593/111593 [01:24<00:00, 1319.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbe85f-034a-4618-a73b-eec9c42f17fd",
   "metadata": {},
   "source": [
    "### PyTorch models expect tensors as inputs. Converting your data into tensors makes it compatible with the model.\n",
    "#### input_ids: A 2D tensor of shape (num_sentences, MAX_SENTENCE_LENGTH) representing tokenized and padded sentences.\n",
    "#### attention_masks: A 2D tensor of the same shape, indicating which tokens are real and which are padding.\n",
    "#### labels: A 1D tensor of shape (num_sentences,) containing binary classification labels.\n",
    "#### This step ensures that all your preprocessed data is ready for use in a PyTorch-based BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_452306/1380106709.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]  # Convert to tensor list\n",
      "/tmp/ipykernel_452306/1380106709.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = [torch.tensor(mask, dtype=torch.float) for mask in attention_masks]  # Convert to tensor list\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "\n",
    "\n",
    "# Ensure input_ids is a list of tensors\n",
    "if not isinstance(input_ids, torch.Tensor):  # Check if input_ids is not already a tensor\n",
    "    input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]  # Convert to tensor list\n",
    "    input_ids = torch.cat(input_ids, dim=0)  # Concatenate tensors along dimension 0\n",
    "\n",
    "# Ensure attention_masks is a list of tensors\n",
    "if not isinstance(attention_masks, torch.Tensor):  # Check if attention_masks is not already a tensor\n",
    "    attention_masks = [torch.tensor(mask, dtype=torch.float) for mask in attention_masks]  # Convert to tensor list\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)  # Concatenate tensors along dimension 0\n",
    "\n",
    "# Ensure labels is a tensor with the correct dtype\n",
    "if not isinstance(labels, torch.Tensor):  # Check if labels is not already a tensor\n",
    "    labels = torch.tensor(df2['4-way-label'].values, dtype=torch.long)  # Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: \"We have 17 intelligence agencies, civilian and military, who have all concluded that these espionage attacks, these cyberattacks, come from the highest levels of the Kremlin, and they are designed to influence our election.\" || Tweet: 17 intelligence agencies KNOW Putin behind cyber attacks. Trump vehemently defends Putin. #treasonoustrump #Debate\n",
      "Token IDs: tensor([  101,  4861,  1024,  1000,  2057,  2031,  2459,  4454,  6736,  1010,\n",
      "         6831,  1998,  2510,  1010,  2040,  2031,  2035,  5531,  2008,  2122,\n",
      "        21003,  4491,  1010,  2122, 16941, 19321,  8684,  2015,  1010,  2272,\n",
      "         2013,  1996,  3284,  3798,  1997,  1996,  1047, 28578,  4115,  1010,\n",
      "         1998,  2027,  2024,  2881,  2000,  3747,  2256,  2602,  1012,  1000,\n",
      "         1064,  1064,  1056, 28394,  2102,  1024,  2459,  4454,  6736,  2113,\n",
      "        22072,  2369, 16941,  4491,  1012,  8398,  2310, 29122, 28198,  6985,\n",
      "         2015, 22072,  1012,  1001, 14712,  3560, 24456,  2361,  1001,  5981,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Inspects the preprocessed data to ensure that the \n",
    "# tokenization and label preparation steps were done correctly. \n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111593, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "835332c1-3a69-4de8-8793-6b5d18ef96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([111593, 410])\n",
      "attention_masks shape: torch.Size([111593, 410])\n",
      "labels shape: torch.Size([111593])\n",
      "Number of rows in input_ids: 111593\n",
      "Number of rows in attention_masks: 111593\n",
      "Number of rows in labels: 111593\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_ids shape: {input_ids.shape}\")\n",
    "print(f\"attention_masks shape: {attention_masks.shape}\")\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "print(f\"Number of rows in input_ids: {len(input_ids)}\")\n",
    "print(f\"Number of rows in attention_masks: {len(attention_masks)}\")\n",
    "print(f\"Number of rows in labels: {len(labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fad8b-6dde-4d90-91de-ef51b2f6393c",
   "metadata": {},
   "source": [
    "### This code creates a TensorDataset to combine your inputs (input_ids, attention_masks, and labels) into a single dataset and then splits it into training and validation sets with an 80-20 ratio.\n",
    "#### Prepares Data for Training:\n",
    "Organizes the input IDs, attention masks, and labels into a single structure (TensorDataset) for easy handling.\n",
    "#### Ensures Data Splitting:\n",
    "\n",
    "Divides the dataset into training and validation sets to evaluate the model's performance during training.\n",
    "#### Supports Random Sampling:\n",
    "\n",
    "random_split ensures the split is randomized, which is essential to avoid biases in training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b4fe4bd-4490-46c4-957f-1eec5c8a250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89,273 training samples\n",
      "22,320 validation samples\n",
      "Training samples by label:\n",
      "Label 0: 22620\n",
      "Label 1: 23323\n",
      "Label 2: 22034\n",
      "Label 3: 21296\n",
      "Validation samples by label:\n",
      "Label 0: 5655\n",
      "Label 1: 5831\n",
      "Label 2: 5509\n",
      "Label 3: 5325\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "# Group data by label\n",
    "label_indices = {label: [] for label in range(4)}  # Assuming labels are [0, 1, 2, 3]\n",
    "for idx, (_, _, label) in enumerate(dataset):\n",
    "    label_indices[label.item()].append(idx)\n",
    "\n",
    "# Perform stratified split\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for label, indices in label_indices.items():\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42  # 80-20 split\n",
    "    )\n",
    "    train_indices.extend(train_idx)\n",
    "    val_indices.extend(val_idx)\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(train_dataset)))\n",
    "print('{:>5,} validation samples'.format(len(val_dataset)))\n",
    "\n",
    "# Count the number of items in each label for both datasets\n",
    "train_label_counts = {label: 0 for label in range(4)}\n",
    "val_label_counts = {label: 0 for label in range(4)}\n",
    "\n",
    "for idx in train_indices:\n",
    "    label = dataset[idx][2].item()  # Access the label\n",
    "    train_label_counts[label] += 1\n",
    "\n",
    "for idx in val_indices:\n",
    "    label = dataset[idx][2].item()  # Access the label\n",
    "    val_label_counts[label] += 1\n",
    "\n",
    "print(\"Training samples by label:\")\n",
    "for label, count in train_label_counts.items():\n",
    "    print(f\"Label {label}: {count}\")\n",
    "\n",
    "print(\"Validation samples by label:\")\n",
    "for label, count in val_label_counts.items():\n",
    "    print(f\"Label {label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ed6d9-93c0-43a6-ba0c-bb3da4b30f76",
   "metadata": {},
   "source": [
    "#### This code creates DataLoaders for the training and validation datasets to efficiently handle and feed data to the BERT model during training and evaluation. The DataLoaders manage batching, sampling, and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,                     # The validation dataset\n",
    "    sampler=SequentialSampler(val_dataset),  # Sequential sampling for consistent evaluation\n",
    "    batch_size=batch_size            # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a575d-8a36-4a1e-8434-9d98a5780cc5",
   "metadata": {},
   "source": [
    "#### This code initializes a BERT model for a binary classification task, preparing it for fine-tuning on your dataset. It loads the pre-trained BERT model with a classification head (a linear layer) on top and specifies configurations for the model's behavior and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 4, # The number of output labels--4 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e860a5a-5757-4df3-98b3-4dc37100852b",
   "metadata": {},
   "source": [
    "#### This code inspects and prints the parameters of the BERT model. It provides insights into the structure and dimensions of the model's parameters, including the embedding layer, transformer layers, and the classification head (output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (4, 768)\n",
      "classifier.bias                                                 (4,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08a049-5551-4b09-84f9-0c7e8ff2dca1",
   "metadata": {},
   "source": [
    "#### This code sets up an optimizer for training your BERT model. Specifically, it uses the AdamW optimizer, which is recommended for fine-tuning transformer models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                 \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a2e93-ee7c-4fd9-a1dd-35c1571b6640",
   "metadata": {},
   "source": [
    "#### This code sets up a learning rate scheduler to adjust the learning rate during training. Specifically, it uses a linear schedule with a warmup phase, which is recommended for fine-tuning transformer models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "num_warmup_steps = int(0.2 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd493e-c762-4efd-9b1d-c3812d744402",
   "metadata": {},
   "source": [
    "#### This code defines a function, flat_accuracy, to calculate the accuracy of predictions compared to the true labels during model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be0a99-907e-4944-b92c-318d1c980761",
   "metadata": {},
   "source": [
    "#### This code defines a utility function format_time that converts a time duration (in seconds) into a more human-readable string format (hh:mm:ss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  changed from /p2/data -> p2/data\n",
    "SAVE_DIR = 'p2/data/checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4e8cd13-06ba-4d72-9cf4-d50fffa61b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels type: <class 'torch.Tensor'>\n",
      "Labels shape: torch.Size([111593])\n",
      "Labels content: tensor([1, 3, 0,  ..., 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels type:\", type(labels))\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Labels content:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295f4b2-9830-480d-8879-fb450c0c1fc3",
   "metadata": {},
   "source": [
    "### This code is the training and validation loop for fine-tuning a BERT model on your dataset. It performs the following:\n",
    "\n",
    "#### Trains the model over several epochs.\n",
    "#### Evaluates the model on a validation set after each epoch.\n",
    "#### Tracks key metrics such as training/validation loss, validation accuracy, and timing.\n",
    "#### Saves the model's weights (checkpoints) after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,790.    Elapsed: 0:00:34. Training loss. 0.6284258365631104 Num fake examples 632 Num true examples 648\n",
      "  Batch    80  of  2,790.    Elapsed: 0:01:07. Training loss. 0.6181288957595825 Num fake examples 1267 Num true examples 1293\n",
      "  Batch   120  of  2,790.    Elapsed: 0:01:41. Training loss. 0.5902084708213806 Num fake examples 1870 Num true examples 1970\n",
      "  Batch   160  of  2,790.    Elapsed: 0:02:15. Training loss. 0.5768303871154785 Num fake examples 2480 Num true examples 2640\n",
      "  Batch   200  of  2,790.    Elapsed: 0:02:48. Training loss. 0.557111382484436 Num fake examples 3074 Num true examples 3326\n",
      "  Batch   240  of  2,790.    Elapsed: 0:03:22. Training loss. 0.5389530062675476 Num fake examples 3708 Num true examples 3972\n",
      "  Batch   280  of  2,790.    Elapsed: 0:03:54. Training loss. 0.5341712236404419 Num fake examples 4339 Num true examples 4621\n",
      "  Batch   320  of  2,790.    Elapsed: 0:04:27. Training loss. 0.5360602140426636 Num fake examples 4966 Num true examples 5274\n",
      "  Batch   360  of  2,790.    Elapsed: 0:04:59. Training loss. 0.5134756565093994 Num fake examples 5615 Num true examples 5905\n",
      "  Batch   400  of  2,790.    Elapsed: 0:05:31. Training loss. 0.46691131591796875 Num fake examples 6238 Num true examples 6562\n",
      "  Batch   440  of  2,790.    Elapsed: 0:06:05. Training loss. 0.4882505536079407 Num fake examples 6855 Num true examples 7225\n",
      "  Batch   480  of  2,790.    Elapsed: 0:06:38. Training loss. 0.4488045573234558 Num fake examples 7483 Num true examples 7877\n",
      "  Batch   520  of  2,790.    Elapsed: 0:07:12. Training loss. 0.4573972821235657 Num fake examples 8132 Num true examples 8508\n",
      "  Batch   560  of  2,790.    Elapsed: 0:07:45. Training loss. 0.4329650104045868 Num fake examples 8755 Num true examples 9165\n",
      "  Batch   600  of  2,790.    Elapsed: 0:08:19. Training loss. 0.4256090819835663 Num fake examples 9373 Num true examples 9827\n",
      "  Batch   640  of  2,790.    Elapsed: 0:08:52. Training loss. 0.4592536687850952 Num fake examples 9979 Num true examples 10501\n",
      "  Batch   680  of  2,790.    Elapsed: 0:09:26. Training loss. 0.39729130268096924 Num fake examples 10625 Num true examples 11135\n",
      "  Batch   720  of  2,790.    Elapsed: 0:09:59. Training loss. 0.42906835675239563 Num fake examples 11239 Num true examples 11801\n",
      "  Batch   760  of  2,790.    Elapsed: 0:10:33. Training loss. 0.39958351850509644 Num fake examples 11875 Num true examples 12445\n",
      "  Batch   800  of  2,790.    Elapsed: 0:11:06. Training loss. 0.3706647455692291 Num fake examples 12509 Num true examples 13091\n",
      "  Batch   840  of  2,790.    Elapsed: 0:11:40. Training loss. 0.36745166778564453 Num fake examples 13134 Num true examples 13746\n",
      "  Batch   880  of  2,790.    Elapsed: 0:12:13. Training loss. 0.4210337996482849 Num fake examples 13709 Num true examples 14451\n",
      "  Batch   920  of  2,790.    Elapsed: 0:12:47. Training loss. 0.44839340448379517 Num fake examples 14299 Num true examples 15141\n",
      "  Batch   960  of  2,790.    Elapsed: 0:13:20. Training loss. 0.4490860104560852 Num fake examples 14936 Num true examples 15784\n",
      "  Batch 1,000  of  2,790.    Elapsed: 0:13:54. Training loss. 0.39196062088012695 Num fake examples 15553 Num true examples 16447\n",
      "  Batch 1,040  of  2,790.    Elapsed: 0:14:27. Training loss. 0.44193780422210693 Num fake examples 16183 Num true examples 17097\n",
      "  Batch 1,080  of  2,790.    Elapsed: 0:15:01. Training loss. 0.3816220164299011 Num fake examples 16796 Num true examples 17764\n",
      "  Batch 1,120  of  2,790.    Elapsed: 0:15:34. Training loss. 0.42835742235183716 Num fake examples 17424 Num true examples 18416\n",
      "  Batch 1,160  of  2,790.    Elapsed: 0:16:08. Training loss. 0.3927789628505707 Num fake examples 18049 Num true examples 19071\n",
      "  Batch 1,200  of  2,790.    Elapsed: 0:16:41. Training loss. 0.3587847948074341 Num fake examples 18678 Num true examples 19722\n",
      "  Batch 1,240  of  2,790.    Elapsed: 0:17:14. Training loss. 0.41126787662506104 Num fake examples 19304 Num true examples 20376\n",
      "  Batch 1,280  of  2,790.    Elapsed: 0:17:46. Training loss. 0.390985906124115 Num fake examples 19929 Num true examples 21031\n",
      "  Batch 1,320  of  2,790.    Elapsed: 0:18:18. Training loss. 0.35331499576568604 Num fake examples 20533 Num true examples 21707\n",
      "  Batch 1,360  of  2,790.    Elapsed: 0:18:51. Training loss. 0.3569936156272888 Num fake examples 21122 Num true examples 22398\n",
      "  Batch 1,400  of  2,790.    Elapsed: 0:19:24. Training loss. 0.4584938883781433 Num fake examples 21730 Num true examples 23070\n",
      "  Batch 1,440  of  2,790.    Elapsed: 0:19:57. Training loss. 0.3794342875480652 Num fake examples 22385 Num true examples 23695\n",
      "  Batch 1,480  of  2,790.    Elapsed: 0:20:31. Training loss. 0.39776793122291565 Num fake examples 23007 Num true examples 24353\n",
      "  Batch 1,520  of  2,790.    Elapsed: 0:21:04. Training loss. 0.440939724445343 Num fake examples 23610 Num true examples 25030\n",
      "  Batch 1,560  of  2,790.    Elapsed: 0:21:38. Training loss. 0.40169692039489746 Num fake examples 24252 Num true examples 25668\n",
      "  Batch 1,600  of  2,790.    Elapsed: 0:22:11. Training loss. 0.3541717231273651 Num fake examples 24867 Num true examples 26333\n",
      "  Batch 1,640  of  2,790.    Elapsed: 0:22:45. Training loss. 0.4298545718193054 Num fake examples 25482 Num true examples 26998\n",
      "  Batch 1,680  of  2,790.    Elapsed: 0:23:18. Training loss. 0.391573041677475 Num fake examples 26133 Num true examples 27627\n",
      "  Batch 1,720  of  2,790.    Elapsed: 0:23:52. Training loss. 0.35655519366264343 Num fake examples 26754 Num true examples 28286\n",
      "  Batch 1,760  of  2,790.    Elapsed: 0:24:25. Training loss. 0.35855162143707275 Num fake examples 27391 Num true examples 28929\n",
      "  Batch 1,800  of  2,790.    Elapsed: 0:24:59. Training loss. 0.39701858162879944 Num fake examples 28030 Num true examples 29570\n",
      "  Batch 1,840  of  2,790.    Elapsed: 0:25:32. Training loss. 0.3503032922744751 Num fake examples 28633 Num true examples 30247\n",
      "  Batch 1,880  of  2,790.    Elapsed: 0:26:06. Training loss. 0.4378640651702881 Num fake examples 29248 Num true examples 30912\n",
      "  Batch 1,920  of  2,790.    Elapsed: 0:26:39. Training loss. 0.3650974929332733 Num fake examples 29866 Num true examples 31574\n",
      "  Batch 1,960  of  2,790.    Elapsed: 0:27:13. Training loss. 0.42422330379486084 Num fake examples 30472 Num true examples 32248\n",
      "  Batch 2,000  of  2,790.    Elapsed: 0:27:46. Training loss. 0.36657044291496277 Num fake examples 31051 Num true examples 32949\n",
      "  Batch 2,040  of  2,790.    Elapsed: 0:28:20. Training loss. 0.3676387667655945 Num fake examples 31654 Num true examples 33626\n",
      "  Batch 2,080  of  2,790.    Elapsed: 0:28:53. Training loss. 0.4156607985496521 Num fake examples 32211 Num true examples 34349\n",
      "  Batch 2,120  of  2,790.    Elapsed: 0:29:27. Training loss. 0.374941885471344 Num fake examples 32836 Num true examples 35004\n",
      "  Batch 2,160  of  2,790.    Elapsed: 0:30:00. Training loss. 0.3576289415359497 Num fake examples 33455 Num true examples 35665\n",
      "  Batch 2,200  of  2,790.    Elapsed: 0:30:33. Training loss. 0.37640684843063354 Num fake examples 34082 Num true examples 36318\n",
      "  Batch 2,240  of  2,790.    Elapsed: 0:31:04. Training loss. 0.3871530294418335 Num fake examples 34719 Num true examples 36961\n",
      "  Batch 2,280  of  2,790.    Elapsed: 0:31:36. Training loss. 0.3491913676261902 Num fake examples 35354 Num true examples 37606\n",
      "  Batch 2,320  of  2,790.    Elapsed: 0:32:08. Training loss. 0.35490232706069946 Num fake examples 35969 Num true examples 38271\n",
      "  Batch 2,360  of  2,790.    Elapsed: 0:32:40. Training loss. 0.416574090719223 Num fake examples 36619 Num true examples 38901\n",
      "  Batch 2,400  of  2,790.    Elapsed: 0:33:12. Training loss. 0.3985569477081299 Num fake examples 37241 Num true examples 39559\n",
      "  Batch 2,440  of  2,790.    Elapsed: 0:33:33. Training loss. 0.42189231514930725 Num fake examples 37849 Num true examples 40231\n",
      "  Batch 2,480  of  2,790.    Elapsed: 0:33:48. Training loss. 0.3837527632713318 Num fake examples 38451 Num true examples 40909\n",
      "  Batch 2,520  of  2,790.    Elapsed: 0:34:02. Training loss. 0.39416787028312683 Num fake examples 39095 Num true examples 41545\n",
      "  Batch 2,560  of  2,790.    Elapsed: 0:34:17. Training loss. 0.38818076252937317 Num fake examples 39714 Num true examples 42206\n",
      "  Batch 2,600  of  2,790.    Elapsed: 0:34:32. Training loss. 0.4516383111476898 Num fake examples 40353 Num true examples 42847\n",
      "  Batch 2,640  of  2,790.    Elapsed: 0:34:46. Training loss. 0.3625817894935608 Num fake examples 40971 Num true examples 43509\n",
      "  Batch 2,680  of  2,790.    Elapsed: 0:35:01. Training loss. 0.3518325388431549 Num fake examples 41594 Num true examples 44166\n",
      "  Batch 2,720  of  2,790.    Elapsed: 0:35:15. Training loss. 0.38170677423477173 Num fake examples 42240 Num true examples 44800\n",
      "  Batch 2,760  of  2,790.    Elapsed: 0:35:30. Training loss. 0.40201330184936523 Num fake examples 42894 Num true examples 45426\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epcoh took: 0:35:41\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation Loss: 0.39\n",
      "  Validation took: 0:01:41\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,790.    Elapsed: 0:00:14. Training loss. 0.48906588554382324 Num fake examples 644 Num true examples 636\n",
      "  Batch    80  of  2,790.    Elapsed: 0:00:28. Training loss. 0.38585084676742554 Num fake examples 1246 Num true examples 1314\n",
      "  Batch   120  of  2,790.    Elapsed: 0:00:43. Training loss. 0.4404549300670624 Num fake examples 1852 Num true examples 1988\n",
      "  Batch   160  of  2,790.    Elapsed: 0:00:57. Training loss. 0.3835810720920563 Num fake examples 2474 Num true examples 2646\n",
      "  Batch   200  of  2,790.    Elapsed: 0:01:12. Training loss. 0.3890525698661804 Num fake examples 3102 Num true examples 3298\n",
      "  Batch   240  of  2,790.    Elapsed: 0:01:26. Training loss. 0.42883390188217163 Num fake examples 3756 Num true examples 3924\n",
      "  Batch   280  of  2,790.    Elapsed: 0:01:41. Training loss. 0.3523366451263428 Num fake examples 4364 Num true examples 4596\n",
      "  Batch   320  of  2,790.    Elapsed: 0:01:55. Training loss. 0.35080236196517944 Num fake examples 4965 Num true examples 5275\n",
      "  Batch   360  of  2,790.    Elapsed: 0:02:09. Training loss. 0.38400667905807495 Num fake examples 5608 Num true examples 5912\n",
      "  Batch   400  of  2,790.    Elapsed: 0:02:24. Training loss. 0.3523964285850525 Num fake examples 6254 Num true examples 6546\n",
      "  Batch   440  of  2,790.    Elapsed: 0:02:38. Training loss. 0.41837477684020996 Num fake examples 6868 Num true examples 7212\n",
      "  Batch   480  of  2,790.    Elapsed: 0:02:53. Training loss. 0.35493171215057373 Num fake examples 7482 Num true examples 7878\n",
      "  Batch   520  of  2,790.    Elapsed: 0:03:07. Training loss. 0.3564378619194031 Num fake examples 8082 Num true examples 8558\n",
      "  Batch   560  of  2,790.    Elapsed: 0:03:22. Training loss. 0.43412306904792786 Num fake examples 8706 Num true examples 9214\n",
      "  Batch   600  of  2,790.    Elapsed: 0:03:36. Training loss. 0.42025458812713623 Num fake examples 9300 Num true examples 9900\n",
      "  Batch   640  of  2,790.    Elapsed: 0:03:51. Training loss. 0.4130733013153076 Num fake examples 9941 Num true examples 10539\n",
      "  Batch   680  of  2,790.    Elapsed: 0:04:05. Training loss. 0.3374674320220947 Num fake examples 10548 Num true examples 11212\n",
      "  Batch   720  of  2,790.    Elapsed: 0:04:20. Training loss. 0.37965184450149536 Num fake examples 11172 Num true examples 11868\n",
      "  Batch   760  of  2,790.    Elapsed: 0:04:34. Training loss. 0.3857821226119995 Num fake examples 11806 Num true examples 12514\n",
      "  Batch   800  of  2,790.    Elapsed: 0:04:49. Training loss. 0.3598994314670563 Num fake examples 12412 Num true examples 13188\n",
      "  Batch   840  of  2,790.    Elapsed: 0:05:03. Training loss. 0.43683475255966187 Num fake examples 13053 Num true examples 13827\n",
      "  Batch   880  of  2,790.    Elapsed: 0:05:25. Training loss. 0.34846460819244385 Num fake examples 13665 Num true examples 14495\n",
      "  Batch   920  of  2,790.    Elapsed: 0:05:51. Training loss. 0.35942885279655457 Num fake examples 14294 Num true examples 15146\n",
      "  Batch   960  of  2,790.    Elapsed: 0:06:16. Training loss. 0.4056582748889923 Num fake examples 14883 Num true examples 15837\n",
      "  Batch 1,000  of  2,790.    Elapsed: 0:06:42. Training loss. 0.4070267975330353 Num fake examples 15484 Num true examples 16516\n",
      "  Batch 1,040  of  2,790.    Elapsed: 0:07:07. Training loss. 0.37816962599754333 Num fake examples 16115 Num true examples 17165\n",
      "  Batch 1,080  of  2,790.    Elapsed: 0:07:33. Training loss. 0.40728944540023804 Num fake examples 16727 Num true examples 17833\n",
      "  Batch 1,120  of  2,790.    Elapsed: 0:07:59. Training loss. 0.44696980714797974 Num fake examples 17327 Num true examples 18513\n",
      "  Batch 1,160  of  2,790.    Elapsed: 0:08:24. Training loss. 0.3549456000328064 Num fake examples 17950 Num true examples 19170\n",
      "  Batch 1,200  of  2,790.    Elapsed: 0:08:50. Training loss. 0.35731256008148193 Num fake examples 18565 Num true examples 19835\n",
      "  Batch 1,240  of  2,790.    Elapsed: 0:09:15. Training loss. 0.350303590297699 Num fake examples 19213 Num true examples 20467\n",
      "  Batch 1,280  of  2,790.    Elapsed: 0:09:41. Training loss. 0.34971052408218384 Num fake examples 19819 Num true examples 21141\n",
      "  Batch 1,320  of  2,790.    Elapsed: 0:10:06. Training loss. 0.35685622692108154 Num fake examples 20419 Num true examples 21821\n",
      "  Batch 1,360  of  2,790.    Elapsed: 0:10:20. Training loss. 0.36092516779899597 Num fake examples 20985 Num true examples 22535\n",
      "  Batch 1,400  of  2,790.    Elapsed: 0:10:35. Training loss. 0.4536580741405487 Num fake examples 21602 Num true examples 23198\n",
      "  Batch 1,440  of  2,790.    Elapsed: 0:10:53. Training loss. 0.3494267463684082 Num fake examples 22209 Num true examples 23871\n",
      "  Batch 1,480  of  2,790.    Elapsed: 0:11:18. Training loss. 0.39269423484802246 Num fake examples 22863 Num true examples 24497\n",
      "  Batch 1,520  of  2,790.    Elapsed: 0:11:44. Training loss. 0.4509595036506653 Num fake examples 23502 Num true examples 25138\n",
      "  Batch 1,560  of  2,790.    Elapsed: 0:12:09. Training loss. 0.4171280264854431 Num fake examples 24153 Num true examples 25767\n",
      "  Batch 1,600  of  2,790.    Elapsed: 0:12:34. Training loss. 0.40032869577407837 Num fake examples 24783 Num true examples 26417\n",
      "  Batch 1,640  of  2,790.    Elapsed: 0:13:00. Training loss. 0.47179901599884033 Num fake examples 25429 Num true examples 27051\n",
      "  Batch 1,680  of  2,790.    Elapsed: 0:13:25. Training loss. 0.34171873331069946 Num fake examples 26040 Num true examples 27720\n",
      "  Batch 1,720  of  2,790.    Elapsed: 0:13:51. Training loss. 0.3587575852870941 Num fake examples 26666 Num true examples 28374\n",
      "  Batch 1,760  of  2,790.    Elapsed: 0:14:17. Training loss. 0.4032316505908966 Num fake examples 27298 Num true examples 29022\n",
      "  Batch 1,800  of  2,790.    Elapsed: 0:14:42. Training loss. 0.42422810196876526 Num fake examples 27919 Num true examples 29681\n",
      "  Batch 1,840  of  2,790.    Elapsed: 0:15:08. Training loss. 0.3641565442085266 Num fake examples 28550 Num true examples 30330\n",
      "  Batch 1,880  of  2,790.    Elapsed: 0:15:33. Training loss. 0.3810728192329407 Num fake examples 29184 Num true examples 30976\n",
      "  Batch 1,920  of  2,790.    Elapsed: 0:15:58. Training loss. 0.4008018374443054 Num fake examples 29771 Num true examples 31669\n",
      "  Batch 1,960  of  2,790.    Elapsed: 0:16:24. Training loss. 0.3863834738731384 Num fake examples 30409 Num true examples 32311\n",
      "  Batch 2,000  of  2,790.    Elapsed: 0:16:49. Training loss. 0.3925294280052185 Num fake examples 31051 Num true examples 32949\n",
      "  Batch 2,040  of  2,790.    Elapsed: 0:17:15. Training loss. 0.408878356218338 Num fake examples 31666 Num true examples 33614\n",
      "  Batch 2,080  of  2,790.    Elapsed: 0:17:40. Training loss. 0.4135468006134033 Num fake examples 32263 Num true examples 34297\n",
      "  Batch 2,120  of  2,790.    Elapsed: 0:18:06. Training loss. 0.3533729314804077 Num fake examples 32898 Num true examples 34942\n",
      "  Batch 2,160  of  2,790.    Elapsed: 0:18:31. Training loss. 0.4715232253074646 Num fake examples 33555 Num true examples 35565\n",
      "  Batch 2,200  of  2,790.    Elapsed: 0:18:57. Training loss. 0.38803741335868835 Num fake examples 34175 Num true examples 36225\n",
      "  Batch 2,240  of  2,790.    Elapsed: 0:19:22. Training loss. 0.3639369010925293 Num fake examples 34786 Num true examples 36894\n",
      "  Batch 2,280  of  2,790.    Elapsed: 0:19:48. Training loss. 0.351975679397583 Num fake examples 35423 Num true examples 37537\n",
      "  Batch 2,320  of  2,790.    Elapsed: 0:20:13. Training loss. 0.3551250398159027 Num fake examples 36036 Num true examples 38204\n",
      "  Batch 2,360  of  2,790.    Elapsed: 0:20:39. Training loss. 0.37485602498054504 Num fake examples 36667 Num true examples 38853\n",
      "  Batch 2,400  of  2,790.    Elapsed: 0:21:04. Training loss. 0.38441383838653564 Num fake examples 37327 Num true examples 39473\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 20000:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += ((b_labels == 2) | (b_labels == 3)).sum().item()\n",
    "        total_true_examples += ((b_labels == 0) | (b_labels == 1)).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=4).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        if step > 20000:\n",
    "            break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=4).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7478b7d-8807-45ec-8269-c0a3e01091c3",
   "metadata": {},
   "source": [
    "#### This code encodes a single sentence, passes it through the fine-tuned BERT model in evaluation mode, and prints the model's output along with the true label for that sentence. The purpose is to test the model on a specific input sentence and examine its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
